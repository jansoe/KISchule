{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A7_3_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jansoe/KISchule/blob/main/A7_3_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w41-FbpjG4_O"
      },
      "source": [
        "# 7. Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txvHEpLLvMKl"
      },
      "source": [
        "## 7.3 Nachtrainieren von GPT-2\n",
        "\n",
        "GPT-2 wurde auf einem Korpus von Wikipedia und Reddit-Texten trainiert. Dementsprechend folgen die generierten Texte dem Duktus dieser Dokumente.\n",
        "\n",
        "In diesem Notebook wollen wir nun explorieren, wie gut das Finetuning (Nachtrainieren) dieses Modell klappt, wenn wir hierfür eine andere Textdomäne verwenden: Gedichte.\n",
        "\n",
        "Dieses Notebook orientiert sich dabei stark an diesem Blog: https://www.gwern.net/GPT-2 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkqRuEdADjhB"
      },
      "source": [
        "### 7.3.0 Vorbereitungen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLQqNxFsJ6AO"
      },
      "source": [
        "%tensorflow_version 2\n",
        "!pip install transformers==2.4\n",
        "import transformers\n",
        "import torch.utils.data as datatools\n",
        "import torch\n",
        "import gzip, json\n",
        "from pprint import pprint\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpcUDoANBLJK"
      },
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBOi8jopN6_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e716acb2-2378-41a2-a600-414fa7876cd4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWegOnTpD93J"
      },
      "source": [
        "Analog zu Abschnitt 7.0.0 wählen Sie hier Ihren Ordner aus, in dem das nachtrainierte Modell abgespeichert werden soll."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tno1laFHc2QK"
      },
      "source": [
        "# Hier werden wir Daten speicher und laden\n",
        "base_dir = '/content/drive/My Drive/KI-Schule/2 - Textdaten' # dieser Ordner muss in Ihrem Google Drive bereits existieren\n",
        "new_dir = '/GPT-2'\n",
        "model_dir = base_dir + new_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LI1HHe0ETrx"
      },
      "source": [
        "!mkdir '{model_dir}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GZ23-ZQdbmX"
      },
      "source": [
        "!ls '{model_dir}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbPPUC9_weiO"
      },
      "source": [
        "### 7.3.1 Das Modell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ8LtV-QKB5v"
      },
      "source": [
        "model_name_or_path = 'gpt2' # vor dem Nachtrainieren\n",
        "# model_name_or_path = model_dir + '2'  # Letzte Epoche des nachtrainierten Modells\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "model = transformers.AutoModelWithLMHead.from_pretrained(model_name_or_path)\n",
        "model.to('cuda')\n",
        "\n",
        "# Jetzt leiten wir aus den Modell Pfad die letzte Trainingsepoche ab\n",
        "# oder setzen diese auf -1 wenn wir mit dem Modelltraining von vorne beginnen\n",
        "last_epoch = -1 if 'drive' not in model_name_or_path else int(model_name_or_path.split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUUJDNbTxw5v"
      },
      "source": [
        "Jetzt können wir schauen, was das Modell mit einem poetischeren `prompt_text` anfängt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYWYi2oTlNTj"
      },
      "source": [
        "prompt_text = ''' \n",
        "Born out of numbers\n",
        "forced to rhyme!\n",
        "Why\n",
        "'''\n",
        "\n",
        "encoded_prompt = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
        "encoded_prompt = encoded_prompt.to('cuda')\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=encoded_prompt,\n",
        "    max_length=48,\n",
        "    do_sample=True,\n",
        "    temperature=2,\n",
        "    top_p=0.5,\n",
        "    repetition_penalty=2\n",
        ")\n",
        "\n",
        "generated_sequence = output_sequences[0].tolist()\n",
        "text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMzTByaKyHMO"
      },
      "source": [
        "Das Ergebnis sieht eher nicht nach einem Gedicht aus. Deswegen wollen wir versuchen, dem Model mit einem Gedichtkorpus etwas Sprachpoesi zu vermitteln."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3hQc1TwuxTL"
      },
      "source": [
        "### 7.3.2 Der Trainingsdatensatz\n",
        "\n",
        "Wir laden eine kuratierte Liste von frei verfügbaren Gedichten herunter (https://github.com/aparrish/gutenberg-poetry-corpus) und erstellen aus dieser einen Trainingsdatensatz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjlSa0MoN679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "02c3ad52-ae66-4ad7-8aec-6d4ee75b3d9b"
      },
      "source": [
        "!curl -O http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 52.2M  100 52.2M    0     0  58.0M      0 --:--:-- --:--:-- --:--:-- 58.0M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmT8Gvnknu0k",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Funktion zum Konvertierten des Downloads in einfachen Text.\n",
        "def convert_to_plain_txt(infile, outfile):\n",
        "    prev_doc_id = None\n",
        "    with open(outfile, 'w') as outfile:\n",
        "        all_lines = []\n",
        "        for line in gzip.open(infile):\n",
        "            data = json.loads(line.strip())\n",
        "            doc_id = data['gid']\n",
        "            if doc_id != prev_doc_id:\n",
        "                outfile.write('\\n\\n')\n",
        "            text = data['s']\n",
        "            outfile.write(text + '\\n')\n",
        "            prev_doc_id = doc_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8GXnt2IxVoP"
      },
      "source": [
        "# Anwendung der zuvor definierten Funktion\n",
        "convert_to_plain_txt(infile = \"gutenberg-poetry-v001.ndjson.gz\", outfile = 'poems.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICa7JOkNL3c2",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Definition einer Dataset-Klasse, die Text als Token bereitstellt.\n",
        "\n",
        "class TextDataset(datatools.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, file_path, block_size):\n",
        "        directory, filename = os.path.split(file_path)\n",
        "\n",
        "        self.examples = []\n",
        "\n",
        "        # Öffnen der Textdatei\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Umwandeln in numerierte Token\n",
        "        tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "        # Zerlegen in Fragmente der Länge block_size\n",
        "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):  \n",
        "            example = tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size])\n",
        "            self.examples.append(example)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbXD9hLuuX1b"
      },
      "source": [
        "# Entsprechend auch hier die eigentliche Anwendung der Dataset-Klasse\n",
        "dataset = TextDataset(\n",
        "    tokenizer,   \n",
        "    file_path = 'poems.txt',\n",
        "    block_size = 64  # Länge der Textfragmente, die wir lernen zu vervollständigen\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo-feQNdylDR"
      },
      "source": [
        "### 7.3.3 Das Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN46K2MoRgV7"
      },
      "source": [
        "optimizer = transformers.AdamW(model.parameters(), lr=5e-4, eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOrSGOl_zLh9"
      },
      "source": [
        "Wir teilen unseren Datensatz in einen Trainings- und Testdatensatz. Der Fehler auf dem Testdatensatz zeigt uns an, wie gut wir Gedichtzeilen, die nicht im Training vorgekommen sind, vervollständigen können."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZanL_N_xcI_"
      },
      "source": [
        "train_size = int(0.95 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZVCHB_KKuL-"
      },
      "source": [
        "# Ein Dataloader für die Trainingsdaten\n",
        "train_dataloader = datatools.DataLoader(\n",
        "    train_dataset, \n",
        "    shuffle=True, \n",
        "    batch_size=32, \n",
        "    collate_fn=lambda x: torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
        ")\n",
        "len(train_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7S1la_Du2Z-"
      },
      "source": [
        "# Ein Dataloader für die Trainingsdaten\n",
        "test_dataloader = datatools.DataLoader(\n",
        "    test_dataset, \n",
        "    batch_size=32, \n",
        "    collate_fn=lambda x: torch.nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
        ")\n",
        "len(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-PeHB9Ai1oZ",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Definition der Funktion evaluate() zum Berechnen des Testfehlers.\n",
        "\n",
        "def evaluate(dataloader, model):\n",
        "    model.eval()\n",
        "    eval_loss = 0 \n",
        "    for step, batch in enumerate(dataloader):\n",
        "        inputs = batch.to('cuda')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, labels=inputs)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.item()\n",
        "\n",
        "    eval_loss = eval_loss / (step+1)\n",
        "\n",
        "    print(f'Validation loss {eval_loss}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z_3WtRjzHiy"
      },
      "source": [
        "Die Trainingsschleife ähnelt der aus Abschnitt 7.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRwuiW4qeo0o"
      },
      "source": [
        "NUM_TRAIN_EPOCHS = 3\n",
        "\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "for epoch in range(last_epoch+1, last_epoch+1+NUM_TRAIN_EPOCHS):\n",
        "    print(f\"== EPOCH {epoch} started ==\" )\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        inputs = batch.to('cuda')\n",
        "        outputs = model(inputs, labels=inputs)\n",
        "        loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if GRADIENT_ACCUMULATION_STEPS > 1:\n",
        "            loss = loss / GRADIENT_ACCUMULATION_STEPS\n",
        "\n",
        "        loss.backward()\n",
        "        \n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "        if (step+1) % 1000 == 0:\n",
        "            print(f'Train Loss {train_loss/(1000):.3f}')\n",
        "            train_loss = 0\n",
        "\n",
        "\n",
        "    last_epoch = epoch\n",
        "    # Speichern des (Zwischen)-Modell\n",
        "    save_dir = os.path.join(model_dir, str(epoch))\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "    model.save_pretrained(save_dir)\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    torch.save(optimizer.state_dict(), os.path.join(save_dir, \"optimizer.pt\"))\n",
        "\n",
        "    evaluate(test_dataloader, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLQmpe3cGrXw"
      },
      "source": [
        "### 7.3.4 Evaluierung des nachtrainierten Modells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9gEKZwsCb3K"
      },
      "source": [
        "prompt_text = ''' \n",
        "Born out of numbers\n",
        "trained by you.\n",
        "Why\n",
        "'''\n",
        "\n",
        "encoded_prompt = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
        "encoded_prompt = encoded_prompt.to('cuda')\n",
        "\n",
        "output_sequences = model.generate(\n",
        "    input_ids=encoded_prompt,\n",
        "    max_length=48,\n",
        "    do_sample=True,\n",
        "    temperature=5,\n",
        "    top_p=0.5,\n",
        "    repetition_penalty=3\n",
        ")\n",
        "\n",
        "generated_sequence = output_sequences[0].tolist()\n",
        "text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yywF7AvQIvg9"
      },
      "source": [
        "![insitubytes](https://drive.google.com/uc?id=1EAJK7AI9tcZRo3VvYq7vEKGxk7vmK2Ff)"
      ]
    }
  ]
}