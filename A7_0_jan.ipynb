{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A7_0_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jansoe/KISchule/blob/main/A7_0_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE0jsYnzbuKm"
      },
      "source": [
        "# 7. Transformer\n",
        "\n",
        "In der heutigen Notebook-Reihe möchten wir Ihnen verschiedene Transformer-Modelle und deren Anwendung näher vorstellen. Wir starten mit Modellen für das Natural Language Processing (NLP). Diese Modelle werden also so trainiert, dass Sie ein möglichst weitreichendes Sprachverständnis erlernen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXItjGH5FuAf"
      },
      "source": [
        "## 7.0 NLP mit BERT und SQuAD\n",
        "\n",
        "In diesem Notebook werden Sie zwei Varianten der NLP-Modellklasse BERT (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers) kennenlernen, die jeweils auf dem Datensatz SQuAD (**S**tanford **Qu**estion **A**nswering **D**ataset) nachtrainiert wurden.\n",
        "\n",
        "---\n",
        "\n",
        "**Anmerkung**\n",
        "\n",
        "Damit Sie sich in der Welt der Deep-Learning-Modelle zurechtfinden können, müssen Sie mit vielen neuen Abkürzungen umgehen, da heutzutage jedes Projekt ein Akronym beansprucht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoqBiy30wRq4"
      },
      "source": [
        "### 7.0.0 Vorbereitungen\n",
        "\n",
        "Vor den Imports muss zunächst die transformers-Bilbiothek in Version 2.4 installiert werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVGK8-h3KAAM"
      },
      "source": [
        "!pip install transformers==2.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUBZtZ-pSydg"
      },
      "source": [
        "Obwohl wir für das Arbeiten mit Textdaten hautpsächlich das die ML-Bibliothek `PyTorch` in Kombination mit der `transformers`-Bibliothek verwenden, umfasst die Liste der Imports auch ein Modul des Ihnen bekannten ML-Frameworks `tensorflow`. Dieses wird dafür benötigt, den benötigten Datensatz `SQuAD` (Stanford Question Answering Dataset) herunterzuladen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLQqNxFsJ6AO"
      },
      "source": [
        "from pprint import pprint\n",
        "import os\n",
        "%tensorflow_version 2\n",
        "import tensorflow_datasets as tfds\n",
        "import torch\n",
        "import torch.utils.data as datatools\n",
        "import transformers\n",
        "from transformers.data.metrics.squad_metrics import compute_predictions_logits, squad_evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb38rGFOad6B"
      },
      "source": [
        "Sollten Sie Interesse daran haben, welche Hardware Ihnen von Google zur Verfügung gestellt wird, können Sie die folgende Code-Zelle ausführen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8FcYJlq5-qJ"
      },
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLAxsqvwcp5_"
      },
      "source": [
        "Sie werden Zugriff auf Ihr Google Drive benötigen, um bei Bedarf Modelle dort abspeichern und wieder laden zu können."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBOi8jopN6_B"
      },
      "source": [
        "# connect google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcWtb4LKY9zt"
      },
      "source": [
        "Wählen Sie nun einen geeigneten Ordner innerhalb Ihres Google Drives, in dem Sie Daten und Modelle abspeichern möchten.\n",
        "\n",
        " ---\n",
        "\n",
        " **Hinweis**\n",
        "\n",
        "Das Verzeichnis der Variable `base_dir` muss bereits in Ihrem Google Drive existieren! Verwenden Sie `'/content/drive/My Drive'`, wenn Sie keine besondere Ordnerstruktur in Ihrem Google Drive verwenden.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HInNHdw-Wx6e"
      },
      "source": [
        "base_dir = '/content/drive/My Drive/KI-Schule/2 - Textdaten' # dieser Ordner muss in Ihrem Google Drive bereits existieren\n",
        "new_dir = '/squad'\n",
        "model_dir = base_dir + new_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWQu_ANhZMq5"
      },
      "source": [
        "Sollten dieser Ordner noch nicht existieren, können Sie ihn über den folgenden Befehl erstellen. Bei einem erneuten Ausführen erscheint die Meldung, dass das Verzeichnis bereits existiert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm-b5wliZ61M"
      },
      "source": [
        "!mkdir '{model_dir}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-FqMHktYR5-"
      },
      "source": [
        "Mit dem folgenden Befehl können Sie testen, ob das Verzeichnis existiert und welche Dateien und Unterverzeichnisse es enthält. Haben Sie es frisch angelegt, so erzeugt der Befehl keinerlei Ausgabe. Im Laufe dieses und des nächsten Notebooks werden dann verschiedene Dateien und Ordner in dem Verzeichnis angelegt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HIcvFzlYHgs"
      },
      "source": [
        "!ls '{model_dir}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbspcRhJb-i6"
      },
      "source": [
        "### 7.0.1 Laden des Modells\n",
        "\n",
        "Legen Sie nun fest, welches Modell mit welchem Tokenizer verwendet werden soll. Dabei können Sie entweder auf ein vortrainiertes Modell aus der Transfomers-Bibiliothek zurückgreifen oder ein von Ihnen nachtrainiertes aus Ihrem Google Drive laden (siehe A7_1).\n",
        "\n",
        "Beginnen Sie beim ersten Durchlauf mit dem relativ kleinen Modell `distilbert-base-uncased-distilled-squad`. Der zugehörige Download umfasst ca. 265MB. Beim großen BERT sind es bereits 1,34GB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhRFO0Xma4Cv"
      },
      "source": [
        "model_name_or_path = 'distilbert-base-uncased-distilled-squad'  # first run\n",
        "# model_name_or_path = 'bert-large-uncased-whole-word-masking-finetuned-squad'  # Aufgabe 7.0.5\n",
        "# model_name_or_path = model_dir + '/0'  # Aufgabe 7.1.4\n",
        "# model_name_or_path = model_dir + '/1'  # Aufgabe 7.1.4\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "model = transformers.AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFuAVHqPFC6n"
      },
      "source": [
        "### 7.0.2 Qualitative Evaluation\n",
        "\n",
        "In diesem Abschnitt können Sie die Funktionalität des Modells stichprobenartig testen. Die offiziellen Testbeispiele für den Squad-Datensatz samt einer komfortablen Benutzeroberfläche, die zum Durchstöbern einlädt, finden Sie [hier](https://rajpurkar.github.io/SQuAD-explorer/explore/1.1/dev/).\n",
        "\n",
        "Wir definieren zunächst die Funktion `evaluate()`, die drei Parameter verwendet - ein Modell, einen Kontext sowie eine Frage. Als Ausgabe errechnet die Funktion die Antwort des Modells bezogen auf die Frage und den zur Verfügung gestellten Kontext."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85TxuyFH3hPr"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Definition der Funktion evaluate().\n",
        "def evaluate(model, context, question):\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        question, \n",
        "        context, \n",
        "        return_attention_mask = False, \n",
        "        return_tensors = 'pt',\n",
        "        return_token_type_ids = not('distilbert' in model_name_or_path) \n",
        "    )\n",
        "\n",
        "    # fuer jeden Token erhalten wir die Wahrscheinlichkeit dass er Start/Endpunkt der Antwort ist\n",
        "    start_scores, end_scores = model(**inputs)\n",
        "\n",
        "    # Wir wandeln unsere Token-Ids in Token um\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].numpy().squeeze())\n",
        "\n",
        "    # Und setzen die Anwort aus den Token zwischen hoechster Start- und hoechtser Endwahrscheinlichkeit zusammen\n",
        "    answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
        "    # Jetzt muessen wir noch die Wortschnipsel zu Woertern zusammenfuegen\n",
        "    answer = answer.replace(' ##', '')\n",
        "    return answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwpTWpWihgH8"
      },
      "source": [
        "Zunächst verwenden wir eines der offiziellen Testbeispiele, um die Fähigkeiten des Modells kennenzulernen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNZ4ofu56jdV"
      },
      "source": [
        "context = \"\"\"\\\n",
        "The Rhine (Romansh: Rein, German: Rhein, French: le Rhin, Dutch: Rijn) \\\n",
        "is a European river that begins in the Swiss canton of Graubünden in the southeastern Swiss Alps, \\\n",
        "forms part of the Swiss-Austrian, Swiss-Liechtenstein border, Swiss-German and then the Franco-German border, \\\n",
        "then flows through the Rhineland and eventually empties into the North Sea in the Netherlands. \\\n",
        "The biggest city on the river Rhine is Cologne, Germany with a population of more than 1,050,000 people. \\\n",
        "It is the second-longest river in Central and Western Europe (after the Danube), at about 1,230 km (760 mi),\\\n",
        "[note 2][note 1] with an average discharge of about 2,900 m3/s (100,000 cu ft/s).\\\n",
        "\"\"\"\n",
        "\n",
        "question =  \"Where does the Rhine begin?\"\n",
        "# question = \"What is the largest city the Rhine runs through? \"\n",
        "# question = \"What river is larger than the Rhine?\"\n",
        "\n",
        "evaluate(model=model, context=context, question=question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEwWISc29ItO"
      },
      "source": [
        "### 7.0.3 Aufgabe: Qualitative Bewertung mit eigenem Text\n",
        "Definieren Sie mindestens einen eigenen Kontext und stellen Sie dem Modell Fragen dazu. Welche Fragen kann das Model gut beantworten und welche eher nicht? \n",
        "\n",
        "---\n",
        "\n",
        "**Hinweis**\n",
        "\n",
        "Untersuchen Sie nach Möglichkeit auch, inwiefern der Stil von Kontext und Frage im Verhältnis zum ursprünglichen Stil des SQuAD-Datensatzes Auswirkungen auf das Ergebnis hat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7znUhQnd3KF7"
      },
      "source": [
        "# Lösung:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF9n9kq7I8tA"
      },
      "source": [
        "### 7.0.4 Quantitative Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6QmTRch-GXi"
      },
      "source": [
        "Jetzt verwenden wir den gesamten Testdatensatz von SQuAD, um eine quantitative Aussage treffen zu können, wie gut das jeweilige Modell die Fragen aus den Kontexten heraus beantworten kann.\n",
        "\n",
        "Hierfür berechnen wir uns die sogenannte [F1-Score](https://en.wikipedia.org/wiki/F1_score). Die zugehörige Berechnung basiert darauf, welcher Anteil der vom Modell geantworteten Tokens in den richtigen (vorgegebenen) Antworten vorkommen (`Precision`) und welchen Anteil die vom Modell korrekt ermittelten Antwort-Tokens an den vorgegebenen Antwort-Tokens haben (`Recall` oder auch `Sensitivity`). Diese beiden Werte werden so miteinander verrechnet, dass das Ergebnis zwischen 0 (schlechter geht nicht) und 1 (perfekt) liegt.\n",
        "\n",
        "Für die zugehörige Berechnung benötigen wir also die Antworten des Modells auf alle Fragestellungen im Testdatensatz. Diesen müssen wir zunächst herunterladen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjlSa0MoN679",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Testdaten von SQuAD herunterladen und zum Testen vorbereiten.\n",
        "\n",
        "# Here we define the dataset preprocessing\n",
        "dataset_parameter = dict(\n",
        "    doc_stride = 128, # When splitting up a long document into chunks, how much stride to take between chunks.\n",
        "    max_seq_length = 384, # The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated\n",
        "    max_query_length= 64, # The maximum number of tokens for the question. Questions longer than this will be truncated to this length.\"\n",
        "    return_dataset=\"pt\", # we generate data in the pytorch format\n",
        ")\n",
        "\n",
        "cache_file = os.path.join(model_dir, 'test_cache')\n",
        "if not os.path.exists(cache_file):\n",
        "\n",
        "    tfds_examples = tfds.load(\"squad\")\n",
        "\n",
        "    # prepare samples\n",
        "    test_examples = transformers.SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=True)\n",
        "    test_features, test_dataset = transformers.squad_convert_examples_to_features(\n",
        "        examples=test_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        is_training=False,\n",
        "        **dataset_parameter\n",
        "    )\n",
        "    torch.save({\"features\": test_features, \"dataset\": test_dataset, \"examples\": test_examples}, cache_file)\n",
        "\n",
        "else: # load data\n",
        "    features_and_dataset = torch.load(cache_file)\n",
        "    test_features, test_dataset, test_examples = (\n",
        "        features_and_dataset[\"features\"],\n",
        "        features_and_dataset[\"dataset\"],\n",
        "        features_and_dataset[\"examples\"],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63nnB_dlo45h"
      },
      "source": [
        "Für die Netzwerkberechnungen wird PyTorch verwendet. Daher wird zunächst auch ein `DataLoader`-Objekt erstellt, bei dem konfiguriert werden kann, in welcher Form die Daten dem Netzwerk zugänglich gemacht werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7S1la_Du2Z-"
      },
      "source": [
        "# Hier legen wir fest, dass wir immer X Samples (Frage-Antwort-Paare) gleichzeitig vorhersagen.\n",
        "test_dataloader = datatools.DataLoader(\n",
        "    test_dataset, \n",
        "    # Anzahl der Samples pro Batch, aus Performance-Gründen so viele wie auf die GPU passen.\n",
        "    # D.h. 6 bei einer T4 oder P4, 12 bei einer P100.\n",
        "    batch_size = 6\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvbmDXTatNOF"
      },
      "source": [
        "Und schließlich verwenden wir den DataLoader, um nach und nach Modellvorhersagen für alle Fragestellungen der Testdaten berechnen zu lassen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hoxAcOu42AZ"
      },
      "source": [
        "#device = 'cpu'\n",
        "device = 'cuda'\n",
        "\n",
        "_ = model.to(device)\n",
        "\n",
        "model.eval()\n",
        "test_results = []\n",
        "\n",
        "# Für jeden Batch  ...\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "\n",
        "    # ... laden wir die Daten auf die GPU/CPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # ... bereiten die Inputs entsprechend vor (je nach verwendetem Modell)\n",
        "    inputs = {\n",
        "            \"input_ids\": batch[0],\n",
        "            \"attention_mask\": batch[1],\n",
        "        }\n",
        "    if \"distilbert\" not in model_name_or_path:\n",
        "            inputs[\"token_type_ids\"] = batch[2] \n",
        "\n",
        "    # ... und schieben die Daten durch das Netz\n",
        "    # (zum Testen ohne Gradientenberechnung, um effizienter zu sein)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    example_indices = batch[3]\n",
        "    for i, example_index in enumerate(example_indices):\n",
        "        \n",
        "        feature_id = int(test_features[example_index.item()].unique_id)\n",
        "        \n",
        "        # und berechenen den besten Start- und Endpunkt für die Antwort\n",
        "        output = [output[i].detach().cpu().tolist() for output in outputs]\n",
        "        start_logits, end_logits = output\n",
        "        result = transformers.data.processors.squad.SquadResult(feature_id, start_logits, end_logits)\n",
        "        test_results.append(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c29s9wwQxoX9"
      },
      "source": [
        "Aus den Modellvorhesagen müssen nun noch die eigentlichen Antworten generiert werden. Hierfür kann die Methode `compute_predictions_logits()` der Transformers-Bibliothek verwendet werden. Auf die zugehörigen Details gehen wir im Rahmen dieses Notebooks jedoch nicht näher ein und haben die zughörigen Einstellungen der Übersicht halber ausgeblendet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdHQmCkI64gB",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Einstellungen für die Modellvorhersage (pred_params)\n",
        "\n",
        "pred_params = dict(\n",
        "    n_best_size = 2, # The total number of n-best predictions to generate\n",
        "    max_answer_length = 30, # The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.\"\n",
        "    do_lower_case = True,\n",
        "    null_score_diff_threshold = 0,\n",
        "    tokenizer = tokenizer,\n",
        "    output_prediction_file = os.path.join(model_dir, \"predictions.json\"),\n",
        "    output_nbest_file = os.path.join(model_dir, \"nbest_predictions.json\"),\n",
        "    output_null_log_odds_file = None,\n",
        "    verbose_logging = False,\n",
        "    version_2_with_negative = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX9lG-xmyWiW"
      },
      "source": [
        "Mit diesen Einstellungen lassen sich nun die eigentlichen Modellantworten bestimmen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-zxZFP35gYM"
      },
      "source": [
        "predictions = compute_predictions_logits(test_examples, test_features, test_results, **pred_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwq3La0u0Rcu"
      },
      "source": [
        "Schauen wir uns bevor wir den F1-Score berechnen zunächst ein Beispiel für eine Kombinationen aus Kontext, zughöriger Frage sowie den als korrekt gelabelten Antworten und der Modellvorhersage an. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ni8v1yfLOD4y"
      },
      "source": [
        "qa_pair = test_examples[3]  # hier können Sie sich ein Beispiel aussuchen\n",
        "\n",
        "print('== Kontext Text ==')\n",
        "pprint(qa_pair.context_text)\n",
        "print('\\n== Frage ==')\n",
        "print(qa_pair.question_text)\n",
        "print('\\n== Als korrekt markierte Antworten ==')\n",
        "print([a['text'] for a in qa_pair.answers])\n",
        "print('\\n== Vorhergesagte Antwort ==')\n",
        "print(predictions[qa_pair.qas_id])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxBEfFACnn6Y"
      },
      "source": [
        "Schließlich lässt sich die Funktion `squad_evaluate()` der Transformer-Bibliothek dafür verwenden, um unterschiedliche Bewertungen der Leistungsfähigkeit des Modells berechnen zu lassen. Wir picken uns die recht ausssagekräftige F1-Score heraus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GhSt2YkVIyv"
      },
      "source": [
        "scores = squad_evaluate(test_examples, predictions)\n",
        "print(scores['f1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq2fYFspof23"
      },
      "source": [
        "Notieren Sie sich den Score für jedes Modell, für das Sie ihn berechnet haben. Hierfür können Sie das folgende Dictionary verwenden:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-T92OJjauYY"
      },
      "source": [
        "quantitative_results = {\n",
        "    'distilbert-base-uncased-distilled-squad' : None,  # Erster Durchlauf dieses Notebooks\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad' : None,  # Aufgabe 7.0.5\n",
        "    'bert-base-uncased' : None,  # Aufgabe 7.1.\n",
        "}\n",
        "pprint(quantitative_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBU45h4Y8bd2"
      },
      "source": [
        "### 7.0.5 Aufgabe: Das große Modell\n",
        "\n",
        "Das gestete Modell `distilbert-base-uncased-distilled-squad` ist besonders klein und auf Effizienz getrimmt. Größere Modelle haben längere Laufzeiten, sind aber auch performanter. \n",
        "\n",
        "Laden Sie in Abschnitt 7.0.1 das Modell `bert-large-uncased-whole-word-masking-finetuned-squad` und führen Sie ansclhießend die Schritte der Abschnitte 7.0.2 und 7.0.4 für die qualitative und quantitative Evaluation des Modells aus. Welche Unterschiede in der Güte der Ergebnisse beobachten Sie?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1eXhoctpM5A"
      },
      "source": [
        "<bitte ausfüllen>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIyBpLeb0ZE3"
      },
      "source": [
        "![insitubytes](https://drive.google.com/uc?id=1EAJK7AI9tcZRo3VvYq7vEKGxk7vmK2Ff)"
      ]
    }
  ]
}