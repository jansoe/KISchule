{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jansoe/KISchule/blob/main/A3_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX02S6Aq8OxF"
      },
      "source": [
        "#  3. Ein Blick ins Innere der neuronalen Netze\n",
        "\n",
        "Im letzten Aufgabenblatt haben Sie im Schnelldurchlauf die Geschichte der Bilderkennung mit neuronalen Netzen durchlebt. Sie haben kleinere Netze selbst erzeugt und trainiert und große, vortrainierte mit `Keras` heruntergeladen. \n",
        "Doch was passiert in so einem Netzwerk eigentlich? Das einzelne Perzeptron mit zweidimensionalen Inputs kann man noch relativ leicht visualisieren, doch schon bei den kleineren, mehrschichtigen und erst recht bei den riesigen modernen Convolutional-Networks entzieht es sich unserer Vorstellungskraft, welche Rolle einzelne Schichten oder gar einzelne Gewichte spielen.\n",
        "\n",
        "Diese Woche lernen Sie Methoden kennen, mit denen Sie die Aktivität in neuronalen Netzen analysieren können. Im Normalfall besteht die Anwendung der Netze darin, dass man die Paramter eines Modells so anpasst, dass für einen gewissen Input ein erwünschter Output entsteht. Diese Woche drehen wir das Problem um: Wir lassen die Parameter der Netzwerke unverändert, und machen uns am Input zu schaffen.\n",
        "\n",
        "Sie lernen, wie man ein Input-Bild so optimiert, dass bestimmte Zellen besonders stark aktiviert werden. So können wir uns bei verschieden großen Netzwerken ansehen, auf welche Merkmale der Eingabedaten die einzelnen Schichten reagieren. Mit diesem Hintergrundwissen können Sie dann nachvollziehen, wie die [Traumbilder](https://de.wikipedia.org/wiki/DeepDream) der tiefen Netze zustande kommen, die vor einiger Zeit durch die [Medien](https://www.zeit.de/digital/internet/2015-07/neuronale-netzwerke-google-inception) gingen.\n",
        "\n",
        "Im Anschluss sind Sie bereit für Ihren ersten *Hack*: Mit den gleichen Techniken lernen Sie Bilder so zu verändern, dass ein Klassifizierungsnetzwerk diese falsch interpretiert, obwohl für das menschliche Auge kein Unterschied zu korrekt klassifizierten Pendants zu erkennen ist.\n",
        "\n",
        "Als letztes gehen wir dann nochmal auf den Image-Style-Tranfer ein, den Sie bereits aus der letzten Vorlesung kennen. Sie werden sehen, dass auch hier eine Optimierung des Inputs nach bestimmten Kriterien erfolgt. \n",
        "\n",
        "Das Verfahren ist wie gehabt:\n",
        "- Erstellen Sie eine Kopie dieses Notebooks in ihrem Google Drive (vorgeschlagene Umbenennung: \"A3 - Vorname, Nachname\")\n",
        "- Editieren Sie die Text- und Codezellen.\n",
        "- Teilen Sie einen Link zum Kommentieren für Ihr Notebook mit uns \n",
        "\n",
        "---\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "- Wie schon in A2 verwenden wir tensorflow in der aktuellen Version 2. Solange diese Version nicht standardmäßig in Colab importiert wird, müssen wir dies explizit angeben:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7UOfe11d6Xm"
      },
      "source": [
        "# Wir bestehen darauf, dass tensorflow 2 verwendet wird.\n",
        "%tensorflow_version 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq9ontC8lWR0"
      },
      "source": [
        "- Auch für diese Aufgabensammlung sind wir wieder auf die Rechenpower von Google angewiesen und benötigen eine Laufzeitumgebung unter Verwendung einer GPU. Unter Umständen kann es bei der Bereitstellung zu Engpässen kommen. Stellen Sie also zunächst sicher, dass Ihnen eine GPU zugewiesen werden konnte (Sollte dies einmal nicht der Fall sein, bleibt Ihnen zumindest bei den darauf angewiesenen Teilen nichts anderen übrig als das Notebook zu einem späteren Zeitpunkt weiter zu bearbeiten.):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGrYACcmgPk0"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.is_gpu_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMXDG4EPZrZw"
      },
      "source": [
        "## 3.0 Visualisierung des bevorzugten Inputs\n",
        " \n",
        "Wie Sie in der letzten Vorlesung gelernt haben, besteht ein neuronales Netz aus einer Ansammlung von *Zellen* (bei Keras *units* genannt), die miteinander verbunden sind. Jeder Input, der in das Netz gegeben wird, erzeugt in jeder Zelle eine *Aktivierung*, das heißt einen bestimmten Ausgabewert der Zelle. Die Zellen reagieren auf unterschiedliche Eingabedaten unterschiedlich stark, abhängig davon, welche Werte die Verbindungsgewichte während des Trainings angenommen haben. \n",
        " \n",
        "Auf welches Bild - oder allgemein welchen Input - eine Zelle besonders stark reagiert, gibt uns Aufschluss darüber, welche Informationen sie aus dem Input extrahiert und welche Rolle sie bei der Berechnung der Ausgabe des gesamten Netzwerks spielt. Wir können also etwas über die Funktionsweise eines Netzwerks lernen, wenn wir den *Input* so optimieren, dass der *Output* bestimmter Zellen maximiert wird."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF2YvnvNzzLI"
      },
      "source": [
        "### 3.0.0 Erzeugung eines trainierten Netzwerks\n",
        " \n",
        "Um nach bevorzugten Inputs suchen zu können, benötigen wir zunächst ein Netzwerk, das bereits auf eine bestimmte Aufgabe trainiert wurde. Als anschauliches Beispiel benutzen wir hierfür das convolutional network `model3`, das wir im letzten Aufgabenblatt A2 erstellt haben, und trainieren es wieder auf den `MNIST`-Datensatz.\n",
        "\n",
        "In den folgenden Code-Zellen werden die dafür nötigen Module importiert, der Datensatz heruntergeladen, das Netzwerk zusammengestellt und schließlich auf den Trainingsdaten trainiert.\n",
        "\n",
        "---\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "Die genaue Anzahl der Filter und deren Größe kann von denen in A2 abweichen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SqM1bzYeNom"
      },
      "source": [
        "# Import der neben tensorflow benötigten Module\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn_dIlu9d6TZ"
      },
      "source": [
        "# Der MNIST-Datensatz wird heruntergeladen und skaliert.\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# wir skalieren unsere Grauwerte auf -1 bis 1 \n",
        "train_images = (train_images/255 - 0.5) * 2\n",
        "test_images = (test_images/255 -0.5) * 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7pns7Piehxi"
      },
      "source": [
        "# Erzeugung und Training eines convolutional networks wie in A2.\n",
        "input_layer = tf.keras.layers.Conv2D(\n",
        "    input_shape = (28, 28, 1),  # Input-Bildgröße und Anzahl der Farbkanäle\n",
        "    filters = 5,                # wieviele verschiedene Filter verwendet werden\n",
        "    kernel_size = (11, 11),      # Filtergröße\n",
        "    activation= 'relu'\n",
        ")\n",
        "\n",
        "flatten_layer = tf.keras.layers.Flatten()\n",
        "\n",
        "out_layer = tf.keras.layers.Dense( \n",
        "    units=10,                      # 10 units als Output\n",
        "    activation='softmax',          # Die Softmax-Aktivierung zur Klassifikation \n",
        "    name = 'out'\n",
        ")\n",
        "\n",
        "model3 = tf.keras.models.Sequential([\n",
        "    input_layer,\n",
        "    flatten_layer,\n",
        "    out_layer\n",
        "])\n",
        "\n",
        "model3.compile(\n",
        "    loss = 'sparse_categorical_crossentropy',  # Klassifizierungsfehler\n",
        "    metrics = ['accuracy'],                    \n",
        ")\n",
        "\n",
        "tf.keras.utils.plot_model(model3, show_shapes = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVq8ZrCxqTrK"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaGb0JxBehr5"
      },
      "source": [
        "model3.fit(\n",
        "    x = train_images.reshape(-1, 28, 28, 1), \n",
        "    y = train_labels,\n",
        "    batch_size=200,\n",
        "    validation_data = (test_images.reshape(-1, 28, 28, 1), test_labels),\n",
        "    epochs = 10   # Eine Epoche ist abgeschlossen wenn alle Datenpukte einmal trainiert wurden\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrOu1k1fTSYQ"
      },
      "source": [
        "### 3.0.1 Optimierung des Inputs\n",
        "\n",
        "Das Ziel ist es nun, für dieses Netzwerk einen *Input* zu finden, der den *Ouput* einer bestimmten Zelle maximiert. Hierfür müssen wir uns eine Zelle aussuchen, deren Ausgabewert maximiert werden soll. Im diesem Fall ist es zunächst besonders anschaulich, eine Zelle aus der Ausgabeschicht zu wählen, da diese eine bestimmte Ziffer repräsentiert.\n",
        "\n",
        "Die Methode `get_layer()` gibt uns Zugriff auf die einzelnen Schichten eines Modells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHS08Q-KDb0l"
      },
      "source": [
        "inspection_layer = model3.get_layer('out')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNcq0mi5kS1x"
      },
      "source": [
        "Wir können nun auf den Ouput dieser Schicht zugreifen wie auf ein ``numpy ndarray`` und uns eine einzelne Zelle heraussuchen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwobhh2OkTTk"
      },
      "source": [
        "inspection_unit = inspection_layer.output[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg0tdBxNlanA"
      },
      "source": [
        "Nun erzeugen wir ein neues Modell, das die gleichen Eingabeschichten besitzt, als Ausgabe jedoch nur die ausgewählte Zelle verwendet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etttKgD_149_"
      },
      "source": [
        "unit_activation = tf.keras.Model(inputs = model3.input, outputs = inspection_unit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kDAC-pLi3R5"
      },
      "source": [
        "tf.keras.utils.plot_model(unit_activation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUOTLdQn-DGJ"
      },
      "source": [
        "unit_activation.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14L0REuxT_uv"
      },
      "source": [
        "Wir haben nun also das Modell, dessen Output wir mittels eines geeigenten Inputs maximieren wollen. Hierfür erstellen wir eine Eingabevariable, die die passende Größe zu diesem Modell hat. Also letztlich ein Bild, das wir dem Netzwerk als Eingabe präsentieren können und im Rahmen der Optimierung verändern werden.\n",
        "\n",
        "---\n",
        "**Anmerkungen**\n",
        "\n",
        "Diese Eingabevariable kann anfangs mit Zufallszahlen gefüllt sein - wir verwenden hier einheitlich 0.7 für jeden Bildpunkt, damit genügend Aktivität im Netz ausgelöst wird. Diese Mindestaktivität benötigen wir insbesondere bei den im späteren Verlauf der Übung verwendeten tiefen neuronalen Netzen, da wir ansonsten das Problem bekommen können, dass kleine Änderungen in der Eingabevariable zu kleine Auswirkungen auf die Aktivität tieferer Schichten zeigen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAxtsTdmEdD6"
      },
      "source": [
        "offset = .7\n",
        "input_variable = tf.Variable(\n",
        "    initial_value = tf.zeros(shape=(1, 28, 28, 1)) + offset,\n",
        "    trainable=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Z5bRlDqXwS"
      },
      "source": [
        "Unsere `input_variable` ist nun der einzige Teil, der in der anstehenden Optimierung verändert werden soll. Das Interface von Keras ist allerdings für das herkömmliche Trainieren von Netzwerken mit annotierten Daten ausgelegt, also das Optimieren von Gewichten. Daher brauchen wir an dieser Stelle etwas mehr Code, um Keras dazu zu bringen, den *Input* zu optimieren. Hierfür müssen wir unsere eigene *Trainingsschleife* schreiben. In jedem Durchlauf dieser Schleife wollen wir die folgenden Schritte durchführen:\n",
        "\n",
        "1. **Die Berechnung der Ausgabe**: In jedem Schritt wird aus dem aktuellen Stand der Input-Variable die Aktivierung unserer Zielzelle berechnet.\n",
        "\n",
        "2. **Die Berechnung des Fehlers**: Keras ist darauf ausgelegt, *Fehler* zu *minimieren*. In unserem Fall wollen wir aber die Aktivierung der Zelle *maximieren*. Dieses Ziel können wir erreichen, indem wir Keras den *negativen* Output minimieren lassen (also `loss = -output`). \n",
        "\n",
        "3. **Die Berechnung der Richtung**: Jeder Pixelwert beinflusst die Ausgabe des Netzwerks. Je nach Verschaltung der Gewichte kann der Output in Abhängigkeit vom Wert eines Pixels ab- oder zunehmen (oder sich auch so gut wie nicht verändern). Keras berechnet für jeden Pixel die Veränderungsrichtung, die den Fehler verkleinert. Mathematisch bezeichnet man diese Richtungen als *Gradienten*.  \n",
        "\n",
        "4. **Die Veränderung der Input-Variable**: Jeder Pixel des Input-Bildes wird um einen kleinen Betrag in Richtung seines Gradienten verändert.\n",
        "\n",
        "Diese Schritte werden in der Trainigsschleife so oft durchgeführt, bis die Abweichung vom gewünschten Ergebnis klein genug ist oder eine vorher festgelegte Anzahl an Interationen erreicht wurde. Im Folgenden sehen Sie eine mögliche Implementierung dieser Trainigsschleife mit Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uefz-SjcAEav"
      },
      "source": [
        "# Wir wählen als Optimierungsalgorithmus den Adam-Algorithmus.\n",
        "# Hierfür initialisieren ein entsprechendes Optimizer-Objekt.\n",
        "# Dieses sammelt die Gradienten und steuert die Veränderungen der Input-Variable.\n",
        "optimizer = tf.optimizers.Adam()\n",
        "# Die Trainingsschleife beginnt. Es werden 500 Iterationen durchlaufen.\n",
        "for i in range(500):\n",
        "    # Es wird ein GradientTape geöffnet. Damit wird Tensorflow mitgeteilt,\n",
        "    # dass für alle eingerückten Operationen die Gradienten aufgezeichnet \n",
        "    # werden sollen.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Die Aktivierung aus unserem Hilfsnetzwerk. \n",
        "        activation = unit_activation(input_variable)\n",
        "        # Es wird gemittelt, falls mehrere units, ein Kanal oder eine ganze Schicht betrachtet werden sollen.\n",
        "        output = tf.reduce_mean(activation)\n",
        "        # Der Fehler ist die Abweichung vom bestmöglichen Output, also 1 - tatsächlicher Output.\n",
        "        error = 1-output\n",
        "    # Zur Kontrolle lassen wir alle 50 Iterationen den Fehler ausgeben.\n",
        "    if i%20 ==0: print(f'Fehler: {error:.6f}')\n",
        "    # Wenn die Abweichung klein genug ist, können wir abbrechen\n",
        "    if error < 0.0000001:\n",
        "        break\n",
        "    # Andernfalls kann nach einem \"besseren\" Input gesucht werden:\n",
        "    # Das GradientTape berechnet dafür, in welche Richtung (Gradient) die\n",
        "    # Variablen verändert werden müssen, um den Fehler zu verkleinern\n",
        "    gradients = tape.gradient(error, input_variable)       \n",
        "    # Der Optimizer berechnet nun aus den Gradienten ein Update für jeden Pixel     \n",
        "    optimizer.apply_gradients([(gradients, input_variable)])     \n",
        "    # Wir beschränken die Lösung auf Werte zwischen -1 und 1, um den Wertebereich\n",
        "    # der Trainingsdaten nicht zu verlassen.\n",
        "    input_variable.assign(tf.clip_by_value(input_variable, -1, 1))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGwVUX1GNPh4"
      },
      "source": [
        "Nach jeder Iteration nimmt der Fehler ab, d.h. die Ausgabe der ausgewählten Zelle nähert sich 0. Sind wir nahe genug, können uns nun das optimierte Inputbild anzeigen lassen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCG3q5BqAtuu"
      },
      "source": [
        "plt.imshow(np.reshape(input_variable.value(), (28,28)), cmap=plt.cm.binary_r)\n",
        "_ = plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qnhyXNkPsb2"
      },
      "source": [
        "In dem entstandenen Bild können wir mit etwas gutem Willen die Umrisse der zugehörigen Ziffer erkennen. Es ist eine Art \"Mutter aller Dreien\", eine Vorstellung von den Mustern im Eingaberaum, die das Netzwerk während des Trainings bezogen auf die Kategorie $3$ entwickelt hat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLB2oF7oQIvD"
      },
      "source": [
        "### 3.0.2 Aufgabe: Alternative Zahlen\n",
        "\n",
        "Verändern Sie den Code so, dass die Aktivierung für eine andere Zahl optimiert wird, und stellen Sie das entstandene Bild des bevorzugten Inputs dar. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_suEmzlgimTB"
      },
      "source": [
        "## 3.1 Merkmalsvisualisierung in großen Netzen\n",
        "\n",
        "Wir haben zuvor am Beispiel eines einfachen Netzwerks gelernt, wie wir die bevorzugten Merkmale einzelner Zellen visualisieren können. Wir können diese Techniken ebenso auf große, vortrainierte Netzwerke aus `keras.applications` anwenden, um einen Eindruck davon zu bekommen, wie die Informationsverarbeitung in tiefen Netzen vor sich geht, die auf Unmengen von Bildern trainiert wurden.\n",
        "\n",
        "### 3.1.0 Laden des Netzwerks\n",
        "\n",
        "Wir laden diesmal das Netz `InceptionV3` herunter, ein guter Kompromiss zwischen Performance und Rechenaufwand:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iApcxQx2ilBJ"
      },
      "source": [
        "model = tf.keras.applications.InceptionV3(include_top=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHMQTk50jHXa"
      },
      "source": [
        "tf.keras.utils.plot_model(model, dpi=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCmduEhrh_JV"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6A0WBnNh62r"
      },
      "source": [
        "Wie Sie sehen, handelt es sich hierbei wieder um ein riesiges Netzwerk mit fast 22 Millionen Parametern.\n",
        "\n",
        "Die Visualisierung der bevorzugten Merkmale einzelner Zellen oder Schichten funktioniert im Prinzip genauso wie bei dem einfachen Netzwerk, das wir zuvor selbst erstellt haben. Wir wählen eine Zelle aus und optimieren dann das Inputbild so, dass die Aktivierung maximiert wird. Dieses Prinzip kann auch auf ganze Schichten angewendet werden. Hierfür maximieren wir einfach die mittlere Aktivierung der gewählten Schicht."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmG0jVJ4oA2I"
      },
      "source": [
        "### 3.1.0 Definition der Optimierungsfunktion \n",
        "\n",
        "Da wir die Operation mehrfach für verschiedene Fälle anwenden wollen, bietet es sich an, den entsprechenden Code in einer Funktion zu kapseln. Da wir die Methode in diesem Fall auch auf große Netzwerke anwenden wollen, bauen wir in die Funktion einen zusätzlichen Kniff ein: Das Inputbild wird immer wieder zufällig leicht verschoben. Dies macht die Optimierung stabiler und führt zu glatteren Ergebnisbildern. \n",
        "\n",
        "Den Code zum verschieben der Bidler müssen Sie nicht im Einzelnen verstehen. In der Funktion `maximize_activation()` können Sie die einzelnen Schritte wiedererkennen, die wir bereits weiter oben verwendet haben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feTEvIt1-R7y"
      },
      "source": [
        "#@title \n",
        "#@markdown Bitte ausführen: Code zum zufälligen Verschieben eines Bildes.\n",
        "\n",
        "def random_shift(img, maxroll):\n",
        "  # Randomly shift the image to avoid tiled boundaries.\n",
        "  if maxroll > 0:\n",
        "    shift = tf.random.uniform(shape=[2], minval=-maxroll, maxval=maxroll, dtype=tf.int32)\n",
        "    shift_down, shift_right = shift[0], shift[1] \n",
        "    img_shifted = tf.roll(tf.roll(img, shift_right, axis=1), shift_down, axis=0)\n",
        "  else:\n",
        "    img_shifted = img\n",
        "  return img_shifted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CqG6tQ3BPC7"
      },
      "source": [
        "# Wir maximieren wieder genau wie oben, jedoch \"wackeln\" wir mit dem Bild, das\n",
        "# wir optimieren, in jedem Schritt ein Stück hin- und her.\n",
        "# Das führt dazu, dass in unserer Lösung Pixel-Werte nicht allzu stark springen. \n",
        "\n",
        "def maximize_activation(input_variable, activation, steps, max_shift):\n",
        "    \"\"\" Function that optimizes the 'input_variable' to maximize the output of\n",
        "        'activation'.\n",
        "\n",
        "        input_variable:  a tf.variable with shape as expected by 'unit_activation'\n",
        "        activation:      a function that produces an activation value from 'input_variable'\n",
        "        steps:           number of iterations\n",
        "        max_shift:       maximum number of pixels the image is randomly shifted by.\n",
        "\n",
        "        returns: the optimized inut_variable \n",
        "        \"\"\"\n",
        "    optimizer = tf.optimizers.Adam() #learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "    for i in range(steps):  \n",
        "        with tf.GradientTape() as tape:\n",
        "            x_shift = random_shift(input_variable, max_shift)          # Wir schieben unser Bild immer zufällig etwas zur Seite\n",
        "            shifted_activation = activation(x_shift)\n",
        "            error = -tf.reduce_mean(shifted_activation)\n",
        "        if i%100 == 0: \n",
        "            print('Fehler: %.2f'%(error.numpy()))\n",
        "        gradients = tape.gradient(error, input_variable)               # Der Gradient gibt an, wie ich eine Variable (hier input_variable) verbessern muss, um den Fehler zu reduzieren\n",
        "        optimizer.apply_gradients([(gradients, input_variable)])       # Der optimizer verbessert input_variable in Richtung des Gradienten\n",
        "       \n",
        "        input_variable.assign(tf.clip_by_value(input_variable, -1, 1)) # Hier beschränken wir unsere Lösung auf Werte zwischen -1 und 1 wie in einem \"richtigem\" Bild\n",
        "    \n",
        "    return input_variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2I-rWzUl6Of"
      },
      "source": [
        "### 3.1.1 Optimierung einzelner Filter\n",
        "\n",
        " Wie zuvor suchen wir uns eine Zelle aus einer Schicht aus dem Netzwerk aus, für die wir das Eingabebild optimieren wollen. Diesmal nehmen wir eine der früheren Schichten, um die grundlegenden Merkmale zu untersuchen, mit denen das Netzwerk arbeitet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDjGA4NvjU94"
      },
      "source": [
        "inspection_layer = model.get_layer('mixed1')\n",
        "inspection_unit = inspection_layer.output[:, 9, 9, 30] # irgendeine Zelle aus der Schicht\n",
        "unit_activation = tf.keras.Model(inputs = model.input, outputs = inspection_unit)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-zX8849mpwB"
      },
      "source": [
        "Wir definieren nun wieder eine Inputvariable und wenden unsere Funktion darauf an:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viPQJu-tnQpi"
      },
      "source": [
        "offset = 0.\n",
        "input_variable = tf.Variable(\n",
        "    initial_value = tf.zeros(shape=(1, 200, 200, 3)) + offset,\n",
        "    trainable=True\n",
        ")\n",
        "input_variable = maximize_activation(input_variable, unit_activation,steps=2000,max_shift=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c090kORqO9c"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow((np.squeeze(input_variable.value())+1)/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6o9QaKZowUN"
      },
      "source": [
        "Wir sehen, dass die von uns gewählte Zelle auf bestimmte Strukturen im Input besonders stark reagiert. In den niedrigeren Schichten der Bilderkennungsnetze reagieren die Zellen, ähnlich wie in biologischen visuellen Systemen, auf einfache Merkmale wie Kanten und Linien in bestimmten Ausrichtungen. \n",
        "\n",
        "Es fällt auf, dass das Bild nur in einem kleinen Bereich Strukturen aufweist, während der Rest vom Optimierungsalgorithmus nicht verändert wurde. Das liegt daran, dass wir eine einzelne *Position* dieses Filters aus dem convolutional network betrachtet haben und der Input für alle Positionen außerhalb des durch diese Filterposition beschriebenen \"rezeptiven Feldes\" die Aktivierung der gewählten Zelle beeinflussen und somit auch keine Gradienten aufweisen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdaYanzELuYD"
      },
      "source": [
        "### 3.1.2 Maximierung ganzer Kanäle \n",
        "\n",
        "Unsere Funktion für die Maximierung der Aktivierung haben wir so geschrieben, dass sie auch mit mehreren Zellen gleichzeitig zurechtkommt. Anstatt einer einzelnen Position eines Filters können wir also auch den ganzen Kanal maximieren, also sämtliche Positionen eines bestimmten Filters. Hierfür müssen wir lediglich in unserer Schicht anstelle der konkreten Position (`[:,9,9,30]` - Pixelposition 9,9 des 30. Filters) den ganzen Bereich auswählen (`[:,:,:,30]` - alle Positionen des 30. Filters).\n",
        "\n",
        "Wir definieren also mit dem neuen Bereich unserer `inspection_layer` eine neue `channel_activation`-Funktion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXuMgd3cyLz0"
      },
      "source": [
        "inspection_channel = inspection_layer.output[:, :, :, 30] # ganzer Kanal, vergleichen Sie mit dem Index weiter oben\n",
        "channel_activation = tf.keras.Model(inputs = model.input, outputs = inspection_channel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyiBL7RJrmuX"
      },
      "source": [
        "Und wenden wie zuvor unsere Funktion darauf an:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFSxXB41E-6d"
      },
      "source": [
        "offset = 0.\n",
        "input_variable = tf.Variable(\n",
        "    initial_value = tf.zeros(shape=(1, 200, 200, 3)) + offset,\n",
        "    trainable=True\n",
        ")\n",
        "input_variable = maximize_activation(input_variable, channel_activation, 2000, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB-PNespNXlA"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow((np.squeeze(input_variable.value())+1)/2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRzrYmOmxMhc"
      },
      "source": [
        "Das optimale Bild besteht nun aus einer sich wiederholenden Struktur, deren Elemente dem Muster des vorherigen Bildes entsprechen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuWDb11ex-cq"
      },
      "source": [
        "### 3.1.3 Aufgabe: Visualisierung anderer Kanäle der gleichen Schicht\n",
        "\n",
        "Visualisieren Sie den optimalen Input für einen oder mehrere andere Kanäle der `mixed1`-Schicht."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcPJws6aydZC"
      },
      "source": [
        "# Lösung:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AykYQHIl5yi"
      },
      "source": [
        "### 3.1.4 Aufgabe: Visualisierung der Kanäle einer tieferen Schicht\n",
        "\n",
        "Visualisieren Sie den optimalen Input für einen oder mehrere Kanäle aus einer tieferen Schicht (mixed2, mixed3, ... mixed10) \n",
        "\n",
        "---\n",
        "**Anmerkungen**\n",
        "\n",
        "Achtung: Je tiefer die Schicht, desto schwieriger wird die Optimierung! Sollte die Optimierung keine zufriedenstellenden Ergebnisse liefern, können Sie versuchen, einen höheren `offset` zu verwenden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tUbquxil7wL"
      },
      "source": [
        "# Lösung:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N17nNqa5LLuF"
      },
      "source": [
        "## 3.2 Deep Dream\n",
        "\n",
        "2015 veröffentlichte Google einige Artikel mit Bildern, die als die *Träume* tiefer Netze bezeichnet wurden. Wie Sie sich mittlerweile vielleicht denken können, beruhen diese Bilder auf ähnlichen Techniken wie die, die Sie gerade kennegelernt haben. Anstelle eines leeren Bildes, wird die Optimierung hier allerdings mit einem beliebigen Foto begonnen. Die Aktivität des Netzwerks wird also in eine bestimmte Richtung geleitet. Aus dieser Augangsposition erzeugt man durch das Maximieren der Aktivität bestimmter Schichten also so etwas wie Assoziationen, die das Netzwerk gelernt hat.\n",
        "\n",
        "### 3.2.0 Inputbild\n",
        "\n",
        "Wir laden also zunächst ein Bild herunter, wie Sie es im zweiten Aufgabenblatt gelernt haben:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG-iGLLclTcQ"
      },
      "source": [
        "!wget -O wolken.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/b/bb/Wolken_%C3%BCber_Boksee.jpg/256px-Wolken_%C3%BCber_Boksee.jpg\n",
        "wolken = plt.imread('wolken.jpg')\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(wolken)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qVfoRqu1CEB"
      },
      "source": [
        "Damit das `Inception`-Netzwerk das Bild interpretieren kann, muss es noch durch das zugehörige Preprocessing geschoben werden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eVYGT0czK_V"
      },
      "source": [
        "wolken_pp = tf.keras.applications.inception_v3.preprocess_input(wolken)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kI0qnE21Xl2"
      },
      "source": [
        "### 3.2.0 Definition der Optimierungsschicht\n",
        "\n",
        "Im Unterschied zu den vorherigen Abschnitten wird diesmal eine ganze Schicht des Netzwerks maximiert. Ansonsten ist der Code für die Aktivierungsfunktion wie gehabt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kol6jwDwmHHO"
      },
      "source": [
        "inspection_layer = model.get_layer('mixed3')\n",
        "layer_activation = tf.keras.Model(inputs = model.input, outputs = inspection_layer.output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIE82HQn13x_"
      },
      "source": [
        "Unsere `input_variable` wird dieses Mal mit dem Bild initialisiert:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBbf9t1e13-Y"
      },
      "source": [
        "input_variable = tf.Variable(\n",
        "    initial_value = wolken_pp[np.newaxis],\n",
        "    trainable=True\n",
        ")\n",
        "input_variable.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2WZ-Nl62DdV"
      },
      "source": [
        "### 3.2.1 Optimierung\n",
        "\n",
        "Bei der Optimierung wird ein zusätzlicher Trick angewandt: Nach einigen Iterationen wird das Bild jeweils vergrößert. Das hat zur Folge, dass das Netzwerk Details auf unterschiedlichen Größenskalen in das Bild hineinassoziert, und wurde aus rein ästhetischen Gründen so gemacht.\n",
        "\n",
        "Im Code haben wir daher eine zusätzliche Schleife über die Anzahl der vorgenommenen Vergrößerungen. In jedem Durchlauf wird unsere Funktion zur Maximierung der Aktivierung ausgeführt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rVc_F2KmDpi"
      },
      "source": [
        "zoom = 1.3 # Vergrößerungsfaktor\n",
        "zoom_steps = 3 # wie oft vergrößert werden soll\n",
        "\n",
        "for n in range(zoom_steps):\n",
        "    # In jedem zoom_step vergrößern wir unser Bild\n",
        "    print('=== Zoom-Schritt: ', n, '===')\n",
        "    if n > 0: # zoom into image\n",
        "        old_shape = np.array(input_variable.numpy().shape[1:3])  \n",
        "        new_shape = tf.cast(old_shape*zoom, tf.int32)\n",
        "        input_variable = tf.Variable(initial_value = tf.image.resize(input_variable, new_shape))\n",
        "\n",
        "    input_variable = maximize_activation(input_variable, layer_activation, 1000, 100)\n",
        "\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(tf.keras.preprocessing.image.array_to_img((input_variable[0]+1)/2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUdR_wW_3iDi"
      },
      "source": [
        "### 3.2.2 Aufgabe: Vergrößerungsfaktor und Zoom-Schritte\n",
        "\n",
        "Welchen Einfluss haben Vergrößerungsfaktor und Anzahl der Zoom-Schritte auf das resultierende Bild? Verändern Sie hierfür die Anzahl der Zoomschritte und den Vergrößerungsfaktor. Probieren Sie einige Kombinationen aus. Was geschieht für viele Schritte mit kleinem Faktor, für wenige Schritte mit großem Faktor?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMVAJx_p4q8F"
      },
      "source": [
        "# Lösung:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAoaA9uE4tZJ"
      },
      "source": [
        "### 3.2.3 Aufgabe: Eigenes Bild verwenden\n",
        "\n",
        "Laden Sie ein anderes Bild Ihrer Wahl herunter und wenden Sie das Deep-Dreaming darauf an. Experimentieren Sie auch mit anderen Schichten im Netzwerk für die Optimierung."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnaa2dl65BZc"
      },
      "source": [
        "# Lösung:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfA6hSfG1x3o"
      },
      "source": [
        "## 3.3 Adversarial Training\n",
        "\n",
        "In den vorangegangenen Abschnitten haben wir gelernt, wie wir ein neuronales Netzwerk auf eine Art und Weise benutzen können, für die es ursprünglich nicht gedacht war. Wir gehen jetzt noch einen Schritt weiter und starten einen *Angriff* auf unser Netzwerk mit Hilfe des sogenannten *Adversarial Training* (*Gegnerisches Trainieren*). Hierbei werden gezielt Inputs generiert, die ein bestimmtes Netzwerk aus dem Konzept bringen sollen. \n",
        "\n",
        "Wir können ein Bild so verändern, dass es von einem Bilderkennungsnetzwerk nicht mehr erkannt wird, obwohl für das menschliche Auge kaum ein Unterschied zu sehen ist. Hierfür benutzen wir ganz ähnliche Techniken wie zuvor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVHzDOV58WOM"
      },
      "source": [
        "### 3.3.0 Ungehinderte Klassifikation\n",
        "\n",
        "Wir wollen zunächst wieder ein vortrainiertes Netzwerk herunterladen und es auf ein Bild anwenden. Diesmal nehmen wir das `MobileNetV2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyV_exCnmduD"
      },
      "source": [
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input, decode_predictions\n",
        "\n",
        "model = MobileNetV2(include_top=True, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA8YGNqj8rZG"
      },
      "source": [
        "Außerdem benötigen wir natürlich wieder ein Bild, das wir klassifizieren wollen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE-s0Ul67FJT"
      },
      "source": [
        "!wget -O example.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Grand_Ducal_Police_car_%28Ford%29_in_Luxembourg_City.jpg/320px-Grand_Ducal_Police_car_%28Ford%29_in_Luxembourg_City.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikoFAL9_86l5"
      },
      "source": [
        "Das Bild wird wie immer zunächst dem netzwerkspezifischen Preprocessing unterzogen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUol5sIi1gvV"
      },
      "source": [
        "img = tf.keras.preprocessing.image.load_img('./example.jpg', target_size=(224,224))\n",
        "\n",
        "img_pp = preprocess_input(tf.keras.preprocessing.image.img_to_array(img))\n",
        "img_pp = img_pp[None, ...]\n",
        "\n",
        "img_pp.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85eKxmTO7xY8"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "plt.imshow(img_pp.squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vq0d2J-W40mz"
      },
      "source": [
        "prediction = model.predict(img_pp)\n",
        "decode_predictions(prediction, top=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yxuIl0V9KzT"
      },
      "source": [
        "Das Netzwerk erkennt richtig, dass es sich um ein Polizeifahrzeug handelt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPWXCx3o9Vg6"
      },
      "source": [
        "### 3.3.1 Störung der Klassifikation\n",
        "\n",
        "Zuvor haben wir ein Inputbild dahingehend verändern wollen, dass die Aktivierung einer bestimmten Zelle maximiert wird. Umgekehrt ist es natürlich auch möglich, die Aktivierung einer bestimmten Zelle zu *minimieren*. Bei der Klassifikation *gewinnt* die Zelle mit dem höchsten Output. In diesem Fall war es die Zelle, die beim Training dem Begriff `police_van` zugeordnet war. \n",
        "\n",
        "Wollen wir die Klassifikation stören, so können wir mit unserer Methode also die Richtung ermitteln, in die jeder Bildpunkt verändert werden muss, um den Output der `police_van`-Zelle zu reduzieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biLTaZ6s5ah3"
      },
      "source": [
        "def get_gradient_direction(model_input, label):\n",
        "\n",
        "    loss_function = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        model_output = model(model_input)\n",
        "        loss = loss_function(label, model_output)\n",
        "\n",
        "    # Get the gradients of the loss w.r.t to the input image.\n",
        "    gradient = tape.gradient(loss, model_input)\n",
        "    # Get the sign of the gradients to create the perturbation\n",
        "    gradient_direction = tf.sign(gradient)\n",
        "\n",
        "    return gradient_direction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6koa8C8-2Km"
      },
      "source": [
        "Wir wenden diese Funktion nun auf die ``police_van``-Zelle an. Als Anfangswert wird der Variablen unser Polizeiautobild gegeben. Sie errechnet uns dann für jeden Pixel eine Richung und gibt diese zurück."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqa_cF8fBrH0"
      },
      "source": [
        "label_id = 734 #police_van_index (https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)\n",
        "\n",
        "x = tf.Variable(\n",
        "    initial_value = img_pp,\n",
        "    trainable=True\n",
        ")\n",
        "\n",
        "gradient_direction = get_gradient_direction(x, label_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_HiZ7os_TGu"
      },
      "source": [
        "Diese Richtungen könne wir als Bild darstellen lassen. Für das menschliche Auge ist hier keine offensichtliche Struktur erkennbar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0VLoBdO_TlF"
      },
      "source": [
        "plt.imshow(gradient_direction.numpy().squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JszcTkeI_lRQ"
      },
      "source": [
        "Wir können nun diese Optimierungsrichtung auf unser ursprüngliches Bild addieren. Hierbei verwenden wir einen kleinen Faktor, so dass das Ausgangsbild möglichst wenig gestört wird. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V29JeJeECEZW"
      },
      "source": [
        "adversarial_img = (img_pp + 0.005 *gradient_direction.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UJ-D2biDxDe"
      },
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "plt.imshow(adversarial_img.squeeze())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyC1jbGx_-0g"
      },
      "source": [
        "Unser Bild sieht quasi unverändert aus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLcuE3t7D2bB"
      },
      "source": [
        "prediction = model.predict(adversarial_img)\n",
        "decode_predictions(prediction, top=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZi3Mzb2AEoV"
      },
      "source": [
        "Das Netzwerk erkennt es jedoch jetzt nicht mehr als Polizeifahrzeug sondern meint ein Rennauto vor sich zu haben. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKwLN3K-AqOX"
      },
      "source": [
        "### 3.3.2 Aufgabe: Weitere Störung\n",
        "\n",
        "Suchen Sie sich auf [github](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a) den Index für die Klasse `racer` und stören Sie auch diese Klasse im `adversarial_img`. Was passiert mit dem Klassifizierungsergebnis?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOHJfXwLLl6h"
      },
      "source": [
        "# Lösung:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5O4gU6zmBZMO"
      },
      "source": [
        "### 3.3.4 Aufgabe: Eigenes Beispiel\n",
        "\n",
        "Laden Sie ein anderes Bild Ihrer Wahl herunter und stören Sie die Klassifikation. Für der Auswahl eines geeigneten Bildes können Sie sich von den unterschiedlichen Klassen, auf die das Netzwerk trainiert wurde, inspirieren lassen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrmIpWDMBrJd"
      },
      "source": [
        "# Lösung:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tac1HAAsLMUz"
      },
      "source": [
        "## 3.4 OPTIONAL: Image style transfer revisited\n",
        "\n",
        "Im Notebook A1 haben Sie bereits den Image Style Transfer bzw. konkreter den Neural Image Style Transfer (NIST) kennengelernt. Dabei benutzten wir eine Funktion, die aus einem *Style*- und einem *Contentbild* ein neues Bild erzeugte, ohne uns damit auseinanderzusetzen, was im Inneren der Funktion geschieht. Wir werden nun sehen wie mit den Techniken, die Sie im Rahmen dieses Notebooks kennengelernt haben, auch der NIST umgesetzt werden kann.\n",
        "\n",
        "Die konkrete Implementierung des NIST ist relativ komplex. Dieser Teil des Notebooks ist daher optional. Machen Sie sich also keine Sorgen, wenn Sie hier nicht ganz durchblicken oder wenn Ihnen der bisherige Stoff schon genug zu Schaffen gemacht hat. \n",
        "\n",
        "Wir beginnen wieder damit, uns jeweils ein Bild für den *Style* und den *Content* herunterzuladen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2XQ9hqKG82Y"
      },
      "source": [
        "content_path = tf.keras.utils.get_file('content.png', 'https://www.stnds.de/damfiles/article_img_12/was-wir-foerdern/programme/link/LINK_Logo_WEB_RGB.png_200-1e31a9f7a822254b191526ea16cd0c79.png')\n",
        "style_path = tf.keras.utils.get_file('style.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZI7ak1SRDbK"
      },
      "source": [
        "content_img = tf.keras.preprocessing.image.load_img(content_path)\n",
        "style_img = tf.keras.preprocessing.image.load_img(style_path)\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(content_img)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(style_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4H-6nFr5Rpt"
      },
      "source": [
        "Der NIST beruht auf der Beobachtung, dass in einem ganz bestimmten vortrainierten Netzwerk, dem `VGG19`, in ganz bestimmten Schichten der Stil eines Bildes kodiert ist, während der Bildinhalt (da dieser noch abstrakter ist) in einer anderen, höheren Schicht abgebildet wird. \n",
        "\n",
        "Die Autoren der ursprünglichen [Studie](https://arxiv.org/abs/1508.06576) kamen daraufhin auf die Idee, dass man ein Bild so optimieren kann, dass es in den Style-Schichten eine ähnliche Aktivität erzeugt wie *ein* Bild, während in der Content-Schicht die Aktivierung eines *anderen* Bildes nachgeahmt wird.   \n",
        "\n",
        "Die Funktionsweise des NIST ist also in gewisser Weise ein Artefakt genau dieses Netzwerks und konnte bisher nicht in dieser Form in anderen Netzen wiederholt werden.\n",
        "\n",
        "Wir laden uns zunächst das Netz `VGG19` herunter. Dieses ist wie gewohnt in `tf.keras.applications` zu finden. Wir verwenden eine vortrainierte Instanz des Netzwerks, indem wir den Parameter `weights` entsprechend spezifizieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nZt7bcARvaR"
      },
      "source": [
        "from tensorflow.keras.applications import vgg19\n",
        "model = vgg19.VGG19(include_top=False, weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap0JV3VXSZAd"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnTCv6QS7a4E"
      },
      "source": [
        "Wie wir sehen hat das Modell etwas mehr als 20 Millionen Parameter. Diese wollen wir aber nicht trainieren - hier wird wieder lediglich der *Input* optimiert.\n",
        "\n",
        "Der [Publikation](https://arxiv.org/abs/1508.06576) können wir die Schichten des Netzwerkes entnehmen, in der *Style* und *Content* repräsentiert sind."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESr2sCv9S5LJ"
      },
      "source": [
        "# Content layer where will pull our feature maps\n",
        "content_layers = ['block5_conv2'] \n",
        "\n",
        "# Style layer of interest\n",
        "style_layers = [\n",
        "    'block1_conv1',\n",
        "    'block2_conv1',\n",
        "    'block3_conv1', \n",
        "    'block4_conv1', \n",
        "    'block5_conv1'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0TzWlIe7-QI"
      },
      "source": [
        "Wir benötigen nur diesen Teil des Netzwerkes. Wir können uns also ein neues Modell erzeugen, das uns genau die gewünschten Schichtaktivitäten als Output gibt.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrUbk-MCsavn"
      },
      "source": [
        "layer_outputs = [model.get_layer(name).output for name in style_layers + content_layers]\n",
        "model = tf.keras.Model([model.input], layer_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMDVzbWE8gUE"
      },
      "source": [
        "Im Folgenden schreiben wir eine Funktion, die aus der Ausgabe des Modells die *Style*- und *Content*-Komponenten extrahiert. Leider ist es nicht ganz so einfach, eine geeignete aktivitätsabhängige Repräsentation des Bildstils zu erhalten. Hierfür wird anhand der Aktivität der Styleschichten die sogenannte [Gramsche Matrix](https://de.wikipedia.org/wiki/Gramsche_Determinante) berechnet, welche die Korrelationen zwischen den einzelnen Filtern misst. Hierüber müssen Sie sich im Detail keine Gedanken machen..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikUTM6KvTAzY",
        "cellView": "form"
      },
      "source": [
        "#@title \n",
        "#@markdown Bitte ausführen: Code zum Extrahieren der Stil- und Inhaltsmerkmale eines Bildes.\n",
        "# Der Stil eines Bildes in Faltungsnetzen kann durch die Korrelation der \n",
        "# Filter beschrieben werden.\n",
        "\n",
        "# Funktion zum Berechnen der Korrelation zwischen einzelnen Filtern in einem Array\n",
        "def gram_matrix(array):\n",
        "\n",
        "  result = tf.linalg.einsum('bijc,bijd->bcd', array, array)\n",
        "  input_shape = tf.shape(array)\n",
        "  num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
        "  \n",
        "  return result/(num_locations)\n",
        "\n",
        "# Funktion zum Extrahieren der Style- und Contentbeschreibung aus dem Output \n",
        "def seperate_style_and_content(outputs):\n",
        "\n",
        "    style_outputs  = outputs[:len(style_layers)]\n",
        "    style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n",
        "\n",
        "    content_outputs = outputs[len(style_layers):]\n",
        "\n",
        "    return style_outputs, content_outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR7XcIfPvHMN"
      },
      "source": [
        "Wie Sie es schon zuvor gesehen haben, müssen wir die Bilder noch einer netzwerkspezifischen Vorverarbeitung unterziehen. Anschließend können wir die Zielvorgaben der *Style*- und *Content*merkmale anhand unserer beiden Bilder errechnen lassen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRnpU6s-RXDJ"
      },
      "source": [
        "max_size = 512 # setzt die Größe fest, auf die wir unser Bild skalieren\n",
        "\n",
        "# Input Vorbereitung für das Style Image\n",
        "# 1. Bild in Array umwandeln\n",
        "style_img_array = tf.keras.preprocessing.image.img_to_array(style_img)\n",
        "# 2. Bild auf maximale Größe skalieren\n",
        "style_img_array = tf.image.resize(style_img_array, (max_size, max_size), preserve_aspect_ratio=True)\n",
        "# 3. Gleiche Vorverarbeitung anwenden wie im Training\n",
        "style_img_prepro = vgg19.preprocess_input(style_img_array)\n",
        "# 4. Eine neue Dimension hinzufügen, damit das Bild die Standardform (1, Höhe, Breite, Farbkanäle) hat\n",
        "style_img_prepro = style_img_prepro[tf.newaxis, :]\n",
        "\n",
        "style_image_output = model(style_img_prepro)\n",
        "style_image_style, style_image_content = seperate_style_and_content(style_image_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpDudA3DATu4"
      },
      "source": [
        "Das gleiche setzen wir jetzt für das Contentbild um:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-AWbO8RRu9m"
      },
      "source": [
        "content_img_array = tf.keras.preprocessing.image.img_to_array(content_img)\n",
        "content_img_array = tf.image.resize(content_img_array, (max_size, max_size), preserve_aspect_ratio=True)\n",
        "content_img_prepro = vgg19.preprocess_input(content_img_array)\n",
        "content_img_prepro = content_img_prepro[tf.newaxis, :]\n",
        "\n",
        "content_image_output = model(content_img_prepro)\n",
        "content_image_style, content_image_content = seperate_style_and_content(content_image_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZx56OzNO62P"
      },
      "source": [
        "An diesem Punkt stehen nun zwei Zielvorgaben für den NIST fest: In der Variable `style_image_style` sind die stilrepräsentierenden (verrechneten) sowie in `content_image_content` die den grundsätzlichen Bildinhalt repräsentierden Aktivitätsmuster gespeichert, die es nun gleichzeitig mit einem zu optimierenden Input zu erreichen gilt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exuLGG4Ov0Yt"
      },
      "source": [
        "### 3.4.0 Definition des Fehlers \n",
        "\n",
        "Unser Ziel ist also ein Bild, das den *Content* des Content-Bildes wiederspiegelt und den *Style* des Style-Bildes (also die entsprechenden Aktivitätsmuster in Netzwerk auslöst). Der Fehler setzt sich somit zusammen aus den Abweichungen\n",
        "- zwischen den Style-Merkmalen von Ziel-Bild und Style-Bild sowie\n",
        "- zwischen den Content-Merkmalen von Ziel-Bild und Content-Bild."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYQoC69BTi64",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Definition des Fehlers\n",
        "\n",
        "style_weight = 1e-2\n",
        "content_weight = 1e4\n",
        "\n",
        "def style_content_loss(outputs):\n",
        "    \n",
        "    style_output, content_output = seperate_style_and_content(outputs)\n",
        "    \n",
        "    style_loss = tf.add_n(\n",
        "        [tf.reduce_mean((style_output[i] - style_image_style[i])**2) \n",
        "         for i in range(len(style_layers))]\n",
        "    )\n",
        "    style_loss *= style_weight / len(style_layers)\n",
        "\n",
        "    content_loss = tf.add_n(\n",
        "        [tf.reduce_mean((content_output[i] - content_image_content[i])**2) \n",
        "         for i in range(len(content_layers))]\n",
        "    )\n",
        "    content_loss *= content_weight / len(content_layers)\n",
        "    \n",
        "    loss = style_loss + content_loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CFyBJXvzwl_"
      },
      "source": [
        "### 3.4.1 Optimierung\n",
        "\n",
        "Jetzt müssen wir wieder einen Optimierer sowie die Startversion des zu optimierenden Eingabebildes initialisieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxEbKMbHZEDi"
      },
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "input_variable = tf.Variable(\n",
        "    initial_value = content_img_array[tf.newaxis, :] / 255,    # wir initalisieren unsere Variable mit dem Content-Bild\n",
        "    trainable=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGYltgnE0e45"
      },
      "source": [
        "Schließlich fehlt uns noch die zugehörige Optimierungsschleife. Wir können das Eingabebild schrittweise optimieren, indem wir dessen Content- und Style-Abstände zu den enstprechenden Bildern verkleinern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-akCqH3VSOM"
      },
      "source": [
        "for i in range(300):  \n",
        "    with tf.GradientTape() as tape:\n",
        "        prepro_input = input_variable * 255\n",
        "        prepro_input = vgg19.preprocess_input(prepro_input)\n",
        "        output = model(prepro_input)\n",
        "        loss = style_content_loss(output)\n",
        "    print('.', end='')\n",
        "    gradients = tape.gradient(loss, input_variable)            # der Gradient gibt an, wie ich eine Variable (hier x) verändern muss, um den Fehler zu verkleinern\n",
        "    optimizer.apply_gradients([(gradients, input_variable)])   # der Optimizer verändert x in Richtung des Gradienten\n",
        "    input_variable.assign(tf.clip_by_value(input_variable, 0, 1))  \n",
        "    # Zwischenergebnis anzeigen\n",
        "    if i%100 == 5:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(tf.keras.preprocessing.image.array_to_img(input_variable[0]))\n",
        "# Endergebnis anzeigen\n",
        "display.clear_output(wait=True)\n",
        "display.display(tf.keras.preprocessing.image.array_to_img(input_variable[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb712IuzX9RH"
      },
      "source": [
        "### 3.4.2 NIST mit eigenen Bildern\n",
        "\n",
        "Jetzt können Sie die hier vorgestelle Impelmentierung der Stilübertragung für Paarungen selbst ausgesuchter Bilder anwenden, indem Sie für diese die Vorgaben bzgl. Style- und Contentbild berechnen lassen und den Input (mit dem Contentbild initialisiert) entsprechend optimieren lassen."
      ]
    }
  ]
}