{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A7_2_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jansoe/KISchule/blob/main/A7_2_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAHQ5ZQKGuv4"
      },
      "source": [
        "# 7. Transformer\n",
        "\n",
        "Innerhalb von A7_0 und A7_1 haben Sie mit unterschiedlichen Varianten von BERT-Netzwerken gearbeitet, die für NLP-Problemstellungen trainiert worden sind. Jetzt widmen wir uns in ähnlicher Weise den Text generierenden Modellen der GPT-2 Familie: Zunächst betrachten wir Beispiele von trainierten GPT-2-Netzen. Im anschließenden Notebook A7_3 können Sie dann versuchen, die Fähigkeiten eines GPT-2-Modells über geziehltes Nachtrainieren an speziellere Anforderungen anzupassen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOl7oE1LGx-g"
      },
      "source": [
        "## 7.2 Textgenerierung mit GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqzS10aGC-cb"
      },
      "source": [
        "### 7.2.0 Vorbereitungen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLQqNxFsJ6AO"
      },
      "source": [
        "%tensorflow_version 2\n",
        "!pip install transformers==2.4\n",
        "import transformers\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aAKNZ3dDDYW"
      },
      "source": [
        "### 7.2.1 Das Modell\n",
        "\n",
        "Hier können Sie das Modell auswählen, mit dem Text generiert werden soll. Am besten geeignet ist zur Zeit die GPT-2 Familie, von der es unterschiedlich komplexe Modelle gibt: `gpt2`, `gpt2-medium`, `gpt2-large`, `gpt2-xl`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ8LtV-QKB5v"
      },
      "source": [
        "model_name_or_path = 'gpt2'\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "model = transformers.AutoModelWithLMHead.from_pretrained(model_name_or_path)\n",
        "_ = model.to('cuda') # Modell auf GPU schieben"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJRZqqVYd_kC"
      },
      "source": [
        "Als nächsten wählen Sie einen Text, mit dem das Modell \"angefüttert\" wird ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GBm519Fh8U7"
      },
      "source": [
        "prompt_text = 'What is artificial Intelligence? It '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QntI_wJ2Xtpr"
      },
      "source": [
        "... und tokenisieren diesen Text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I31ox-FiE3Rh"
      },
      "source": [
        "encoded_prompt = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
        "encoded_prompt = encoded_prompt.to('cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbBk0kwTR6pv"
      },
      "source": [
        "Der Textgenerierung liegt zugrunde, dass das Modell in jedem Schritt für jedes Token im Vokabular eine Wahrscheinlichkeit angibt, mit der das Modell das jeweilige Token für die nächste Position vorhersagt. Nun können wir 'greedy' einfach immer das wahrscheinlichste Token auswählen und zur nächsten Position schreiten. Aber dies führt oft zu durchwachsenen Resultaten (siehe https://arxiv.org/abs/1904.09751).\n",
        "\n",
        "Für eine verbesserte Qualität der erzeugten Texte bieten sich verschiedene Einstellungsmöglichkeiten an:\n",
        "- `repetition_penalty`: Tokens, die bereits generiert wurden, werden um diesen Faktor unwahrscheinlicher\n",
        "- `do_sample`: Es wird nicht das wahrscheinlichste Token genommen sondern anhand der vorhergesagten Wahrscheinlichkeitsverteilung ein Token zufällig ausgewählt\n",
        "  - `temperature`: Je höher die Temperatur, desto höher die Chance, dass auch unwahrscheinliche Tokens ausgewählt werden\n",
        "  - `top_`: Einschränkungen für die zur Verfügung stehenden Tokens:\n",
        "    - `top_k`: es werden nur die wahrscheinlichsten k Tokens zugelassen\n",
        "    - `top_p`: von den wahrscheinlichsten Tokens werden so viele zugelassen, dass deren Gesamtwahrscheinlichkeit `p` nicht überschreitet.\n",
        "\n",
        "Ausführlichere Informationen finden Sie auch [hier](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VRN_y7zFGBT"
      },
      "source": [
        "output_sequences = model.generate(\n",
        "    input_ids=encoded_prompt,\n",
        "    max_length=128, # Wieviele Token sollen maximal generiert werden\n",
        "    repetition_penalty=3,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k= None, # set to None to ignore\n",
        "    top_p=0.5, # set to None to ignore or 0 < p <= 1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV63J3PjGNA6"
      },
      "source": [
        "generated_sequence = output_sequences[0].tolist()\n",
        "text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "pprint(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whgyeP4Lfi2I"
      },
      "source": [
        "### 7.2.2 Aufgabe: Exploration der Parameter\n",
        "\n",
        "Experimentieren Sie mit GPT-2 und den Modellfähigkeiten zur Textgenerierung. Wie können Sie die überzeugensten Texte generieren? Halten Sie Ihre Beobachtungen in geeigneter Weise hier im Notebook fest.\n",
        "\n",
        "- Was beobachten Sie, wenn Sie `repetition_penalty = None` und `do_sample = False` setzten?\n",
        "- Was passiert mit `do_sample = True` und verschiedenen Werten von `temperature` and `top_p`?\n",
        "- Welchen Einfluss hat die Länge des Eingegebenen `prompt_text`?\n",
        "- Was passiert, wenn Sie mit `prompt_text = '<|endoftext|>'` startet?\n",
        "\n",
        "---\n",
        "**Hinweis**\n",
        "\n",
        "Bei `<|endoftext|>` handelt es sich um ein Spezialtoken, das GPT2 den Start eines neuen Dokuments signalisiert. Als prompt_text verwendet muss GPT2 somit ohne Kontext - also frei - Text generieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZBGVQFVItPF"
      },
      "source": [
        "![insitubytes](https://drive.google.com/uc?id=1EAJK7AI9tcZRo3VvYq7vEKGxk7vmK2Ff)"
      ]
    }
  ]
}