{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A5_1_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jansoe/KISchule/blob/main/A5_1_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX02S6Aq8OxF"
      },
      "source": [
        "# 5. Automatische Textvergleiche (klassisch) - Teil 2\n",
        "\n",
        "Das Verfahren ist wie gehabt:\n",
        "- Erstellen Sie eine Kopie dieses Notebooks in ihrem Google Drive (vorgeschlagene Umbenennung: \"A5_Teil1 - Vorname, Nachname\")\n",
        "- Editieren Sie die Text- und Codezellen.\n",
        "- Schicken Sie uns einen Freigabelink zum Kommentieren Ihres Notebooks an"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU-tPjIgSgeP"
      },
      "source": [
        "## 5.1. Arbeiten mit Daten von Wikipedia\n",
        "\n",
        "### Allgemeine Vorbereitungen\n",
        "\n",
        "Zunächst importieren wir alle Bibliotheken, die im Verlauf dieses Notebooks benötigt werden.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6sSUrDuDXkX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import gensim\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKp6DjYcR9UB"
      },
      "source": [
        "Binden Sie nun Ihr Google Drive ein, damit Sie dort Daten ablegen und zu einem späteren Zeitpunkt (d.h. in einer anderen Session) auf diese erneut zugreifen können."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PhFLT3Fa9v5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "24e31c49-2eb2-4bfe-f74c-fecdedf974bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-pOm2UTXuYC"
      },
      "source": [
        "Ebenso wie im vorherigen Notebook benötigen wir für die anschließenden Schritte einen Tokenizer. Wir definieren dafür die gleiche Funktion wie schon zu Beginn von Abschnitt 5.1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwWHHVvZxFVC"
      },
      "source": [
        "def tokenizer(text):\n",
        "    tokens = list(gensim.utils.simple_tokenize(text))\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVvu1XG0Nhuo"
      },
      "source": [
        "Schließlich wählen Sie ein geeignetes Verzeichnis Ihrer Wahl in Ihrem Google Drive aus (und erstellen dieses bei Bedarf), in das die für diesen Abschnitt benötigten Textdaten ablegt werden können.\n",
        "\n",
        "Die drei Variablen `article_file`, `filtered_file` und `corpus_file` werden auch an dieser Stelle bereits definiert, damit Sie u.U. direkt zu Abschnitt 5.4.2 springen können."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP9LPsQyK5oB"
      },
      "source": [
        "# Bitte die Variable path entsprechend anpassen\n",
        "import os\n",
        "path = \"/content/drive/My Drive/KI-Schule/2 - Textdaten/wiki/\"\n",
        "os.makedirs(path)\n",
        "\n",
        "# Definition von Dateinamen, die in 5.4.1 zum Speichern und in 5.4.2 zum Laden verwendet werden\n",
        "article_file = path + \"article_dict.json\"\n",
        "filtered_file = path + \"wiki_filtered.dict\"\n",
        "corpus_file = path + \"wiki_corpus.mm\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDfCa5-_Wsp"
      },
      "source": [
        "### Daten herunterladen und vorbereiten\n",
        "\n",
        "Jetzt führen wir ein Skript aus, das einen Teil des sogenannten Wikipedia-Dumps (Rohdaten verschiedener Wikipedia-Artikel) in verwertbaren Text umwandelt, z.B. durch das Entfernen von HTML-Formatierungen. Dazu muss der entsprechende Dump zuerst heruntergeladen und in Ihrem Google Drive abgelegt werden.\n",
        "\n",
        "Um die Daten ohne Umwege direkt in Google Drive kopieren zu können, kann das Programm `wget` verwendet werden. Sollten Sie Probleme mit dem Download bekommen, müssen Sie evtl. einen aktuellen Pfad für die Variable `link_address` angeben. An die Rohdaten von Wikipedia und somit die entsprechende Link-Adresse gelangen Sie z.B. über diesen [LINK](https://dumps.wikimedia.org/dewiki/latest/).\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "- **Achtung: Das Ausführen der Code-Zellen dieses Abschnitts benötigt sehr viel Zeit!**\n",
        "- Da Sie die Ergebnisse in Ihrem Google Drive abspeichern werden, sollten Sie die zugehörigen Befehle jedoch lediglich einmal ausführen müssen. Anschließend können Sie auch nach einer Bearbeitungspause nach 5.4.0 direkt mit 5.4.2 weitermachen.\n",
        "- Wir empfehlen Ihnen, alle Schritte dieses Abschnitts \"in einem Rutsch\" (in einer Session) abzuarbeiten. Rechnen Sie hierfür mit einem ungefähren Zeitaufwand von 1h."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuKTquJO8NiI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "6cf8b5e6-42e9-482f-9114-aa59c519d49b"
      },
      "source": [
        "link_address = \"https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles-multistream1.xml-p1p262467.bz2\"\n",
        "dump_file = path + \"wiki_dump.bz2\"\n",
        "!wget \"{link_address}\" -O \"{dump_file}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-25 15:24:43--  https://dumps.wikimedia.org/dewiki/latest/dewiki-latest-pages-articles-multistream1.xml-p1p262467.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 469739844 (448M) [application/octet-stream]\n",
            "Saving to: ‘/content/drive/My Drive/KI-Schule/2 - Textdaten/wiki/wiki_dump.bz2’\n",
            "\n",
            "/content/drive/My D 100%[===================>] 447.98M  5.02MB/s    in 89s     \n",
            "\n",
            "2020-01-25 15:26:13 (5.02 MB/s) - ‘/content/drive/My Drive/KI-Schule/2 - Textdaten/wiki/wiki_dump.bz2’ saved [469739844/469739844]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFJ0103ATHhZ"
      },
      "source": [
        "Jetzt können wir das angesprochene Skript anwenden, dass aus den Rohdaten die verwertbaren Bestandteile extrahiert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GtD1N78ohzp"
      },
      "source": [
        "processed_file = path + \"wiki-latest.json.bz\"\n",
        "!python -m gensim.scripts.segment_wiki -i -f \"{dump_file}\" -o \"{processed_file}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E_bPTGJbHR6"
      },
      "source": [
        "Als nächstes wandeln wir den extrahierten Text in ein Dictionary um, das die jeweiligen Titel als Schlüssel (keys) und die zugehörigen Dokumente als Werte (values) beinhaltet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOiLOaK1axh7"
      },
      "source": [
        "processed_file = path + \"wiki-latest.json.bz\"\n",
        "article_dict = {}\n",
        "with gensim.utils.smart_open(processed_file, 'rb') as f:\n",
        "    for line in f:\n",
        "        article = json.loads(line)\n",
        "\n",
        "        article_dict[article['title']] = '\\n'.join(article['section_texts'])\n",
        "# Ergebnis abspeichern\n",
        "json.dump(article_dict, open(article_file, 'w'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qCUk7jkbwHC"
      },
      "source": [
        "Nun erstellen wir wie schon in Abschnitt 5.1 mit Hilfe des Tokenizers das Token Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg3LH1NEn9HL"
      },
      "source": [
        "token_dictionary = gensim.corpora.Dictionary()\n",
        "cache = []\n",
        "to_process = len(article_dict)\n",
        "for text in article_dict.values():\n",
        "    if to_process % 1000 == 0:\n",
        "        # Füge Cache dem Dictionary hinzu\n",
        "        token_dictionary.add_documents(cache)\n",
        "        cache = []\n",
        "        clear_output()\n",
        "        print(f'{to_process} documents still need processing')\n",
        "    tokenized_text = tokenizer(text)\n",
        "    cache.append(tokenized_text)\n",
        "    to_process -= 1\n",
        "# Ergebnis abspeichern\n",
        "wiki_dict_file = path + \"wiki.dict\"\n",
        "token_dictionary.save(wiki_dict_file)\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7wcSBgxb6WO"
      },
      "source": [
        "Als nächstes filtern wir unser Token Dictionary ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEnVbT7IzKt9"
      },
      "source": [
        "token_dictionary = gensim.corpora.Dictionary.load(wiki_dict_file)\n",
        "token_dictionary.filter_extremes(\n",
        "    no_below=5,     # wir filtern alle Tokens, die seltener als X mal vorkommen \n",
        "    no_above=0.7,   # wir filtern alle Tokens, die in mehr als X Prozent vorkommen\n",
        "    keep_n=500000   # wir behalten maximal 500.000 tokens\n",
        ")\n",
        "# Ergebnis abspeichern\n",
        "token_dictionary.save(filtered_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NDNOfmVc4ll"
      },
      "source": [
        "und erstellen schließlich unseren finalen \"vektorisierten\" Korpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQdyFY_kn52i"
      },
      "source": [
        "corpus_numeric = []\n",
        "to_process = len(article_dict)\n",
        "for text in article_dict.values():\n",
        "    if to_process % 1000 == 0:\n",
        "        clear_output()\n",
        "        print(f'{to_process} documents still need processing')\n",
        "    tokenized_text = tokenizer(text)\n",
        "    corpus_numeric.append(token_dictionary.doc2bow(tokenized_text))\n",
        "    to_process -= 1\n",
        "# Ergebnis abspeichern\n",
        "gensim.corpora.MmCorpus.serialize(corpus_file, corpus_numeric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7THLHaH3xNzj"
      },
      "source": [
        "### Ähnlichkeit von Wikipedia-Artikeln\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "Haben Sie den Abschnitt **Daten herunterladen und vorbereiten** bereits zu einem früheren Zeitpunkt (d.h. in einer anderen Sesssion) ausgeführt, müssen Sie darauf achten, dass Sie nun zunächst die Code-Zellen des Abschnitts **Allgemeine Vorbereitungen** ausführen, um alle benötigten Bibliotheken zu importieren sowie den `Tokenizer` und die Variable `path` zu definieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BblQ2Yk348UX"
      },
      "source": [
        "token_dictionary = gensim.corpora.Dictionary.load(filtered_file)\n",
        "corpus_numeric = gensim.corpora.MmCorpus(corpus_file)\n",
        "article_dict = json.load(open(article_file))\n",
        "\n",
        "titles = list(article_dict.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD71rXI9Ob44"
      },
      "source": [
        "Als eine aussagekräftige numerische Repräsentation verwenden wir das im letzten Notebook vorgestellte TFIDF-Verfahren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB5cfx5-DBz1"
      },
      "source": [
        "tfidf = gensim.models.TfidfModel(corpus_numeric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zeq3nu08eXD_"
      },
      "source": [
        "Das anschließende Berechnen der paarweisen Ähnlichkeiten nimmt ein paar Minuten in Anspruch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW5Srs_WJZvZ"
      },
      "source": [
        "index_wiki = gensim.similarities.SparseMatrixSimilarity(tfidf[corpus_numeric], num_features=corpus_numeric.num_terms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SXh6sozOyaJ"
      },
      "source": [
        "Die großen Vorverarbeitungsschritte sind hiermit abgeschlossen. Nun können wir  Ergebnisse zu verschiedenen Arten von Fragestellungen errechnen lassen, ohne dabei lange Wartezeiten in Anspruch nehmen zu müssen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNEkKDC5eXV1"
      },
      "source": [
        "#### Auffinden von verwandten Artikel zu einem ausgewählten Artikel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KupXPNNlfw0-"
      },
      "source": [
        "Zum Beispiel können wir einen bestimmten Artikel auswählen ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSVUE-k6J1D1"
      },
      "source": [
        "# In unserem Fall sollte der Index zum Turing-Test-Artikel führen\n",
        "doc_number = 19856\n",
        "print(titles[doc_number])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xofcuu8hgLkK"
      },
      "source": [
        "... und uns die zehn ähnlichsten Artikel (basierend auf dem TF/IDF-Ähnlichkeitsmaß) des Korpus ausgeben lassen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tomsgSu9f-6Z"
      },
      "source": [
        "doc_vector = corpus_numeric[doc_number]\n",
        "\n",
        "index_wiki.num_best = 10\n",
        "match = index_wiki[tfidf[doc_vector]]\n",
        "\n",
        "print('Der Artikel', titles[doc_number], 'ähnlich zu:')\n",
        "for ix, score in match:\n",
        "    print(titles[ix], f'{score:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXTY0Cn1epz2"
      },
      "source": [
        "#### Auffinden von passenden Artikeln zu einem Suchwort"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1K1-ScpQAkQ"
      },
      "source": [
        "In diesem Beispiel starten wir mit einem Begriff, um uns die hierzu passensten Artikel heraussuchen zu lassen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFX4siOD_ixE"
      },
      "source": [
        "query =  'Intelligenz' \n",
        "index_wiki.num_best = 2000\n",
        "\n",
        "query_vector = token_dictionary.doc2bow(tokenizer(query))\n",
        "\n",
        "match = index_wiki[query_vector]\n",
        "match_index = [i[0] for i in match]\n",
        "\n",
        "print([titles[ix] for ix in match_index])\n",
        "print(len(match_index), 'matches')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezvuQsXpgJNt"
      },
      "source": [
        "\n",
        "Nun wollen wir anschauen, wie die Artikel, die wir aus unserem Query enthalten haben, untereinander verwandt sind. Hierfür berechnen wir zunächst die Ähnlichkeitsmaße sämtlicher paarweisen Vergleiche der oben ermittelten 2000 Artikel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijAcaeQT_HRb"
      },
      "source": [
        "index_wiki.num_best = None # in diesem Fall ohne Beschränkung der Anzahl ähnlicher Artikel\n",
        "all2all_similarities = index_wiki[tfidf[corpus_numeric[match_index]]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GnsEn1qd1Fz"
      },
      "source": [
        "### Visualisieren von Artikelähnlichkeiten mittels t-SNE\n",
        "\n",
        "Um diese hohe Anzahl an Dokumentenähnlichkeiten zu veranschaulichen, benutzen wir eine `unsupervised learning` Technik namens t-SNE (T-distributed Stochastic Neighbor Embedding). t-SNE wird insbesondere dafür verwendet, hochdimensionale Daten in zwei oder drei Dimensionen zu visualisieren. \n",
        "\n",
        "Vereinfacht gesprochen versucht t-SNE Nachbarschaften eines hochdimensionalen Datensatzes in einer niedrigdimensionalen Repräsentation korrekt abzubilden. Das bedeutet, dass die Distanzen zwischen Punkten, die in der hochdimensionalen Repräsentation nahe beieinander liegen, auch in der niedrigdimensionalen Darstellung möglichst genauso beibehalten werden.\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "Eine detaillierte Erklärung zu t-SNE gibt es [hier](https://www.youtube.com/watch?v=NEaUSP4YerM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBhKsznBMtJv"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "# Die Perplexity gibt an, was als \"Nachbarschaft\" zählt \n",
        "# (je größer, desto mehr Einfluss haben auch weiter entfernte Punkte)\n",
        "tsne = TSNE(\n",
        "    init='pca', \n",
        "    perplexity=10,     \n",
        "    learning_rate=200  \n",
        ")\n",
        "tsne_vis = tsne.fit_transform(all2all_similarities[:, match_index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj3CjcZpRq6M"
      },
      "source": [
        "Für die eigentliche Visualisierung verwenden wir in diesem Fall die Bibliotheken `plotly` und `pandas`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrqSJcTm8_6K"
      },
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "\n",
        "to_plot = pd.DataFrame(tsne_vis, columns=['x', 'y'])\n",
        "to_plot['labels'] = np.array(titles)[match_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCmuBd5JSRgy"
      },
      "source": [
        "Zunächst können wir einen Blick auf die grundsätzliche Struktur der Daten werden. Hier sehen wir, dass jedem Artikel eine zweidimensionale Koordinate (x, y) zugewiesen wurde."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73THGgk5SOtN"
      },
      "source": [
        "to_plot.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMvbXLWWSj6h"
      },
      "source": [
        "Schließlich lassen wir uns einen Scatterplot erstellen, in dem es einige interessante Zusammenstellungen lokaler Cluster zu entdecken gilt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78cb-XAu9Mtu"
      },
      "source": [
        "px.scatter(to_plot, x='x', y='y', hover_name='labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoRMyiSnhdh_"
      },
      "source": [
        "### 5.1.0 Gemeinsamkeiten von Clustern\n",
        "\n",
        "Suchen Sie sich drei lokale Cluster heraus (z.B. mit Bezug zum Thema KI) und versuchen Sie durch Lesen der entsprechenden Wikipedia-Artikel das gemeinsame Thema bzw. die wesentlichen Gemeinsamkeiten zu identifizieren und hier zu beschreiben.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoIqM9lffi0v"
      },
      "source": [
        "<bitte ausfüllen>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJsmJr1SxH2Z"
      },
      "source": [
        "### 5.1.1 High Perplexity\n",
        "Stellen Sie den t-SNE Parameter `perplexity` auf 200 und visualisieren Sie die zugehörigen Nachbarschaftsverhältnisse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFQYG3_tzbfQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOOSJ9cmxuHT"
      },
      "source": [
        "Welche Unterschiede zur ursprünglichen Darstellen beobachten Sie und wie können sie diese erklären?\n",
        "\n",
        "<bitte ausfüllen>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vOsTlCYxucB"
      },
      "source": [
        "### 5.1.2 Low Perplexity\n",
        "Setzen Sie den Parameter `perplexity` auf 1 und visualisieren Sie die zugehörigen Nachbarschaftsverhältnisse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt8pjZB_ziZE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bxCXhrhxuvn"
      },
      "source": [
        "Welche Unterschiede zur ursprünglichen Darstellen beobachten Sie und wie können sie diese erklären?\n",
        "\n",
        "<bitte ausfüllen>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KJsichf2Z2q"
      },
      "source": [
        "![insitubytes](https://drive.google.com/uc?id=1EAJK7AI9tcZRo3VvYq7vEKGxk7vmK2Ff)"
      ]
    }
  ]
}