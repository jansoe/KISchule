{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A5_0_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jansoe/KISchule/blob/main/A5_0_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX02S6Aq8OxF"
      },
      "source": [
        "# 5. Automatische Textvergleiche (klassisch) - Teil 1\n",
        "\n",
        "in den Abschnitten 1 bis 4 haben wir ausschließlich Bilddaten mit unterschiedlichen Methoden des maschniellen Lernens verarbeitet. Jetzt widmen wir uns dem Bereich der Textdaten, bei dem es ganz andere Herausforderungen zu meistern gilt, jedoch ebenso verblüffende Ergebnisse unter Anwendung aktueller Verfahren und hinreichend viel Rechenleistung erreicht werden können.\n",
        "\n",
        "Grundsätzlich verbrauchen Textdaten wesentlich weniger Datenplatz als Bilder. Zum Vergleich: Ein einzelnes, unkomprimiertes HD-Bild (1920x1080 Pixel, 24bit Farbtiefe) verbraucht mit ca. 6MB mehr Speicher als der gesamte Bibeltext ([ca. 4MB im ANSI/ASCII-Format](https://de.wikipedia.org/wiki/Datenmenge)). Auch die komprimierten Bilder einer herkömmlichen 12MP-Kamera können je nach Motiv trotz Kompression auf über 4MB Speicherbedarf pro Einzelaufnahme kommen.\n",
        "\n",
        "Letztlich ist der geringe Speicherbedarf von Text nicht weiter verwunderlich, handelt es sich bei den Schriftzeichen, die unsere Sprache kodieren, bereits um einen verschachtelten Code mit einer entsprechend vergleichsweise hohen Informationsdichte. Und anders als bei der uns zu einem Großteil angeborenen Fähigkeit Bilder interpretieren zu können benötigen wir viele Jahre intensiven Trainings, um auch komplizierte Texte, die über Fachtermini, Mehrsilbenwortungetümer oder auch Verzweigungen, die auch dieses Beispiel eines viel zu sehr in die Länge gezogenen Satzes beinhaltet, verfügen, flüssig lesen zu können.\n",
        "\n",
        "In diesem Notebook werden wir uns zunächst mit einfachen Methoden zur Berechnung von Ähnlichkeitsmaßen zwischen Texten beschäftigen.\n",
        "\n",
        "Das Verfahren ist wie gehabt:\n",
        "- Erstellen Sie eine Kopie dieses Notebooks in ihrem Google Drive (vorgeschlagene Umbenennung: \"A5_Teil1 - Vorname, Nachname\")\n",
        "- Editieren Sie die Text- und Codezellen.\n",
        "- Schicken Sie uns einen Freigabelink zum Kommentieren Ihres Notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8IUFXAF56sx"
      },
      "source": [
        "## 5.0 Numerische Repräsentation von Text und Ähnlichkeitsmaße\n",
        "\n",
        "Nach ein paar hilfreichen Imports erstellen wir uns zunächst einen Datensatz. Diesen gilt es dann in eine geeignete numerische Repräsentation zu wandeln, um darauf basierend im Anschluss unterschiedlich aussagekräftige Ähnlichkeitsmaße zwischen den Beispieltexten des Datensatzes zu berechnen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6sSUrDuDXkX"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk  # natural language toolkit\n",
        "import gensim\n",
        "from pprint import pprint  # pretty print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu3kdeIpyKML"
      },
      "source": [
        "### Ein einfacher Datensatz\n",
        "\n",
        "Wir starten mit einem sehr übersichtlichen Datensatz, der aus lediglich vier kurzen Texten besteht. Es handelt sich dabei um Kurzbeschreibungen der Städte Hannover, Braunschweig, München und Nürnberg - jeweils wenige der ersten Sätze der zugehörigen Wikipedia-Seiten.\n",
        "\n",
        "---\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "- Im Kontext der Textdatenverarbeitung wird eine funktionale (z.B. thematische) Texteinheit als Dokument bezeichnet.\n",
        "- Für die Definition von Zeichenketten (also dem Datentyp `string`), die über mehrere Zeilen reichen, werden in Python wahlweise drei Anführungszeichen (`\"\"\"bla\"\"\"`) oder drei Hochkommata (`'''blub'''`) verwendet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fojJCD0feNqJ"
      },
      "source": [
        "doc1 = \"\"\"Hannover ist die Hauptstadt des Landes Niedersachsen. Der \n",
        "am Südrand des Norddeutschen Tieflandes an der Leine gelegene Ort wurde 1150 \n",
        "erstmals erwähnt und erhielt 1241 das Stadtrecht. Hannover war ab 1636 welfische \n",
        "Residenzstadt, ab 1692 Residenz Kurhannovers, ab 1814 Hauptstadt des Königreichs Hannover, \n",
        "nach dessen Annexion durch Preußen ab 1866 Provinzhauptstadt der Provinz Hannover, \n",
        "nach Auflösung Preußens im August 1946 Hauptstadt des Landes Hannover und nach \n",
        "dessen Fusion mit den Freistaaten Braunschweig, Oldenburg und Schaumburg-Lippe \n",
        "im November 1946 niedersächsische Landeshauptstadt. Seit 1875 Großstadt, \n",
        "zählt Hannover heute mit 538.068 Einwohnern (Ende 2018) zu den 15 \n",
        "einwohnerreichsten Städten Deutschlands.\"\"\"\n",
        "\n",
        "doc2 = \"\"\"Braunschweig ist eine Großstadt im Südosten des Landes Niedersachsen. \n",
        "Mit 248.292 Einwohnern (Stand 31. Dezember 2018) ist sie nach Hannover die \n",
        "zweitgrößte Stadt Niedersachsens. Die kreisfreie Stadt bildet mit den Städten \n",
        "Salzgitter und Wolfsburg eine Regiopolregion und eines der neun Oberzentren des \n",
        "Bundeslandes. Sie ist Teil der im Jahr 2005 gegründeten Metropolregion \n",
        "Hannover-Braunschweig-Göttingen-Wolfsburg. Im Großraum Braunschweig wohnen rund \n",
        "eine Million Menschen.\"\"\"\n",
        "\n",
        "doc3 = \"\"\"München ist die Landeshauptstadt des Freistaates Bayern. Sie ist mit \n",
        "etwa 1,5 Millionen Einwohnern die einwohnerstärkste Stadt Bayerns und (nach Berlin\n",
        "und Hamburg) die nach Einwohnern drittgrößte Gemeinde Deutschlands. Sie bildet \n",
        "das Zentrum der Metropolregion München (rund 6 Millionen Einwohner) und der \n",
        "Planungsregion München (rund 2,9 Millionen Einwohner). München ist eine kreisfreie Stadt. \n",
        "Sie ist Verwaltungssitz des die Stadt umgebenden Landkreises München mit dessen Landratsamt\n",
        "sowie des bayerischen Bezirks Oberbayern und des Regierungsbezirks Oberbayern.\"\"\"\n",
        "\n",
        "doc4 = \"\"\"Nürnberg ist eine fränkische kreisfreie Großstadt im Regierungsbezirk \n",
        "Mittelfranken des Freistaats Bayern. Nürnberg ist mit rund 520.000 Einwohnern nach \n",
        "München die größte Stadt Bayerns, weist dabei ein deutliches Wachstum auf und gehört \n",
        "damit zu den 15 größten Städten Deutschlands. Zusammen mit den direkten Nachbarstädten \n",
        "Fürth, Erlangen und Schwabach bildet Nürnberg mit rund 800.000 Einwohnern eine der \n",
        "drei Metropolen in Bayern. Gemeinsam mit ihrem Umland bilden diese Städte den \n",
        "Ballungsraum Nürnberg, mit über 1,3 Millionen Menschen und das wirtschaftliche \n",
        "und kulturelle Zentrum der knapp 3,6 Millionen Einwohner umfassenden Europäischen \n",
        "Metropolregion Nürnberg, eine der 11 Metropolregionen in Deutschland.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKcnRLlwkpZf"
      },
      "source": [
        "Unsere vier Dokumente können wir in einen geeigneten Container-Datentyp packen, z.B. in eine Liste. Eine solche Sammlung von Dokumenten wird im Fachjargon als Korpus bezeichnet.\n",
        "\n",
        "---\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "Nur zur Erinnerung und Übersicht: Die drei am häufigsten verwendeten Container-Datentypen in Python sind `tuple` (runde Klammern), `list` [eckige Klammern] und `dict` {geschweifte Klammern}."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8zzlhJPeNkq"
      },
      "source": [
        "corpus = [doc1, doc2, doc3, doc4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1gPfH8EjAyb"
      },
      "source": [
        "### Numerische Repräsentation der Texte\n",
        "\n",
        "Da wir die Textdaten automatisiert durchsuchen, vergleichen oder andersartig analysieren wollen, ist es sehr hilfreich, wenn wir eine adäquate numerische Repräsentation verwenden. Diese kann je nach den Problemstellungen, die gelöst werden sollen, variieren.\n",
        "\n",
        "In einem ersten Schritt zur numerischen Darstellung der Texte werden diese in atomare Bestandteile, die auch als **Tokens** bezeichnet werden, zerlegt. Bereits hier gibt es unterschiedliche Möglichkeiten für diese sogenannte \"Tokenisierung\". Eine typische und naheliegende Definition ist es, dass ein Token einem Wort entspricht.\n",
        "\n",
        "---\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "Wir verwenden hier zum Tokenisieren den Standard-Tokenizer der Open-Source-Bibliothek [`Gensim`](https://en.wikipedia.org/wiki/Gensim)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3njxS-Ulghe"
      },
      "source": [
        "def tokenizer(text):\n",
        "    tokens = list(gensim.utils.simple_tokenize(text))\n",
        "    return tokens\n",
        "# Hier kommt eine sogenannte list comprehension zum Einsatz. Das ist eine for-Schleife,\n",
        "# die in eckigen Klammern steht und als Ergebnis eine Liste liefert.\n",
        "corpus_tokenized = [tokenizer(document) for document in corpus]\n",
        "\n",
        "# Beispielhafte Ausgabe der tokenisierten Version des Hannover-Textes\n",
        "pprint(corpus_tokenized[0], compact=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGGmJcYL_vfd"
      },
      "source": [
        "Wie wir sehen beinhalten die Tokens keine Satzzeichen wie Kommas, Punkte oder Bindestriche. Auch Zahlen werden bei der Tokenisierung nicht beachtet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK7ymza3Sckh"
      },
      "source": [
        "### Bag of words\n",
        "\n",
        "Als nächstes ordnen wir im Zuge der numerischen Repräsentation jedem Token eine eineindeutige Zahl zu ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h9hyOo0lVvv"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(corpus_tokenized)\n",
        "pprint(dictionary.token2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IXQgLGMpxiQ"
      },
      "source": [
        "... und erhalten somit als numerische Darstellung unserer tokenisierten Texte eine Liste von Zahlen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Zrx_AnfQq_"
      },
      "source": [
        "pprint(dictionary.doc2idx(corpus_tokenized[0]), compact=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngT2PnDNfHE9"
      },
      "source": [
        "Alternativ können wir uns auch eine sogenannte *Bag of Words* (bow) ausgeben lassen. Dabei wird der numerische Schlüssel eines jeden Tokens zusammen mit seiner Auftrittshäufigkeit innerhalb des Korpus ausgegeben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX7nC5_CfhwR"
      },
      "source": [
        "# bow am Beispiel des Hannover-Textes\n",
        "pprint(dictionary.doc2bow(corpus_tokenized[0]), compact=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Md29AWNTjDd"
      },
      "source": [
        "Wir können uns somit auch anschauen, wie häufig ein jedes Token in den vier Dokumenten auftritt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpXyH-TqlkyD"
      },
      "source": [
        "# Ganzer Korpus numerisiert\n",
        "corpus_numeric = [dictionary.doc2bow(token_list) for token_list in corpus_tokenized]\n",
        "# Kompakte Version auffüllen mit Nullen - daher muss die Methode wissen, wieviele Spalten existieren\n",
        "gensim.matutils.corpus2dense(corpus_numeric, num_terms=len(dictionary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjsGwegWijfX"
      },
      "source": [
        "### Automatischer Textvergleich\n",
        "\n",
        "Mit Hilfe dieser Repräsentation unserer Texte können wir nun effizient mathematische Operationen ausführen lassen, z.B. Ähnlichkeiten zwischen den vier Texten berechnen.\n",
        "\n",
        "Bei der Ähnlichkeitsberechnung wird im wesentlichen verglichen, wie stark sich das Vokabular von zwei Dokumenten überschneidet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaBHHwAVqjNy"
      },
      "source": [
        "similarity_index = gensim.similarities.SparseMatrixSimilarity(corpus_numeric, num_features=len(dictionary))\n",
        "all2all_similarity = similarity_index[corpus_numeric]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnJTH9l6qjZb"
      },
      "source": [
        "plt.imshow(all2all_similarity, cmap='hot')\n",
        "plt.xticks(range(4), ['H', 'Bs', 'M', 'N'])\n",
        "plt.yticks(range(4), ['H', 'Bs', 'M', 'N'])\n",
        "pax = plt.colorbar()\n",
        "pax.set_label('Ähnlichkeitsmaß')\n",
        "_ = plt.title('Vergleich des Vokabulars')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmVuemNbRev2"
      },
      "source": [
        "Schauen wir uns an, welche der Tokens (sprich welche Wörter) am stärksten zu dem hier verwendeten Ähnlichkeitsmaß zwischen den ersten beiden Dokumenten (Hannover und Braunschweig) beitragen. Hierfür definieren wir uns zunächst die Funktion `top_similarity_contribution()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kn6vRrHTqjmh"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Definition von **top_similarity_contributions**\n",
        "\n",
        "def top_similarity_contributions(doc_numeric1, doc_numeric2, n_best=7):\n",
        "    \n",
        "    d1 = {token_id: count for token_id, count in doc_numeric1}\n",
        "    d2 = {token_id: count for token_id, count in doc_numeric2}\n",
        "    both = {}\n",
        "    for token_id in d1:\n",
        "        if token_id in d2:\n",
        "            counts1 = d1[token_id]\n",
        "            counts2 = d2[token_id]\n",
        "            both[token_id] = (counts1, counts2, counts1*counts2)\n",
        "    best = sorted(both.items(), key=lambda x : x[1][2], reverse=True)\n",
        "    print('Stärkste Beiträge zur Ähnlichkeit:')\n",
        "    print('----------------------------------')\n",
        "    for ix in range(min(n_best, len(both))):\n",
        "        token_id, data = best[ix]\n",
        "        print(f'{dictionary[token_id]} (id: {token_id}) hat Score {data[2]:.3f}={data[0]:.2f}*{data[1]:.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlXEXuzJVxwl"
      },
      "source": [
        "Diese können wir jetzt für sämtliche Dokument-Paarungen unseres Korpus verwenden. Für die Paarung Hannover-Braunschweig liefert sie folgendes Ergebnis:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg8ZBqvAiRiE"
      },
      "source": [
        "top_similarity_contributions(corpus_numeric[0], corpus_numeric[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWFPKtreUEC1"
      },
      "source": [
        "Die Texte über Braunschweig und München ähneln sich dagegen wie folgt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5J0CcvgUEW1"
      },
      "source": [
        "top_similarity_contributions(corpus_numeric[1], corpus_numeric[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VEYHbiISNQr"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Wie wir sehen, beruht die Ähnlichkeit der Texte (bzw. der Dokumente) auf dem gemeinsamen Vorkommen von Wörten wie \"der\", \"des\", \"und\" u.ä. Jedoch können wir davon ausgehen, dass diese Wörter keinen Beitrag zum eigenltichen Inhalt liefern. Diese Art von Wörtern wir auch als Stopwörter genannt. Lassen Sie uns deshalb die Analyse ohne Stopwörter wiederholen.\n",
        "\n",
        "Die Bibliothek `nltk` liefert eine Zusammenstellung der Stoppwörter für verschiedenen Sprachen inkl. Deutsch. Hierauf basierend können wir uns eine Hilfsfunktion definieren, die uns angibt, ob es sich bei einem Wort um ein gelistetes Stoppwort handelt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxKP_kEXS3Qh"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('german')\n",
        "def is_not_stopword(word):\n",
        "    return word not in stopwords\n",
        "# Ein kurzer Blick auf alle gelisteten Stopwörter\n",
        "pprint(stopwords, compact=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FweUhTH9XbNC"
      },
      "source": [
        "Diese Funktion können wir jetzt als Filterfunktion verwenden, um die Stopwörter aus unseren Dokumenten heraus zu filtern. Anschließend können wir erneut die Ähnlichkeitsmaße berechnen lassen - dieses Mal entsprechend anhand der gefilterten Dokumente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDYzniVSa_Y-"
      },
      "source": [
        "# Wir filtern die Stopwörter aus unseren Dokumente \n",
        "# und wandeln die restlichen Tokens wieder in Zahlen um\n",
        "corpus_numeric_filtered = [dictionary.doc2bow(filter(is_not_stopword, token_list))\n",
        "                          for token_list in corpus_tokenized]\n",
        "\n",
        "# Jetzt berechnen wir wieder die Ähnlichkeit aller Dokumentenpaarungen.\n",
        "similarity_index2 = gensim.similarities.SparseMatrixSimilarity(corpus_numeric_filtered, num_features=len(dictionary))\n",
        "all2all_similarity_no_stopwords = similarity_index2[corpus_numeric_filtered]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIaQHXJ6qjjT"
      },
      "source": [
        "plt.imshow(all2all_similarity_no_stopwords, cmap='hot')\n",
        "plt.xticks(range(4), ['H', 'Bs', 'M', 'N'])\n",
        "plt.yticks(range(4), ['H', 'Bs', 'M', 'N'])\n",
        "pax = plt.colorbar()\n",
        "pax.set_label('Ähnlichkeitsmaß')\n",
        "_ = plt.title('Vergleich ohne Stopwörter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBj5Lk4iU4Id"
      },
      "source": [
        "Erneut der Blick auf die Wörter, die zur Ähnlichkeit zwischen Hannover und Braunschweig sowie zwischen Braunschweig und München beitragen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh5SzlmZiNin"
      },
      "source": [
        "print(\"HANNOVER - BRAUNSCHWEIG:\")\n",
        "top_similarity_contributions(corpus_numeric_filtered[0], corpus_numeric_filtered[1])\n",
        "print(\"\\nBRAUNSCHWEIG - MÜNCHEN:\")\n",
        "top_similarity_contributions(corpus_numeric_filtered[1], corpus_numeric_filtered[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UgTJ9ENVURT"
      },
      "source": [
        "Wir sehen nun, dass zwar nicht mehr allgemeine Stopwörter einen hohen Einfluss auf das Ähnlichkeitsmaß haben, aber dass es weiterhin korpus-spezifische Wörter gibt (\"Einwohner\", \"Städten\", ...) die wir nicht als inhaltsspezifisch bewerten würden.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7khusx6FWPhw"
      },
      "source": [
        "### TFIDF\n",
        "Eine Möglichkeit, den Einfluss solcher eigentlich unspezifischen Wörter zu verringern und damit die Aussagekraft der Vergleiche zu erhöhen, stellt das TFIDF-Verfahren (Term Frequency - Inverse Document Frequency) dar. Hierbei wird der Informationsgehalt der Wörter nach ihrem Vorkommen in allen Dokumenten des gesamten Korpus gewichtet: Ein Wort, das auch in vielen anderen Dokumenten vorkommt, sagt nur wenig über die Ähnlichkeit von Texten. Ein Wort, das nur in wenigen Dokumenten vorkommt, ist ein stärkerer Indikator dafür, dass sich deren Texte thematisch ähneln. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL_RxWejo4kR"
      },
      "source": [
        "tfidf = gensim.models.TfidfModel(corpus_numeric)\n",
        "print(corpus_numeric[0])\n",
        "print(tfidf[corpus_numeric[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw55hDAgqGb8"
      },
      "source": [
        "similarity_index_tfidf = gensim.similarities.SparseMatrixSimilarity(tfidf[corpus_numeric], num_features=len(dictionary))\n",
        "all2all_similarity_tfidf = similarity_index_tfidf[tfidf[corpus_numeric]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkMriKKHhvwf"
      },
      "source": [
        "plt.imshow(all2all_similarity_tfidf, cmap='hot')\n",
        "_ = plt.xticks(range(4), ['H', 'Bs', 'N', 'M'])\n",
        "_ = plt.yticks(range(4), ['H', 'Bs', 'N', 'M'])\n",
        "pax = plt.colorbar()\n",
        "pax.set_label('Ähnlichkeitsmaß')\n",
        "_ = plt.title('Vergleich mittels TFIDF')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25HvhkhYhvoy"
      },
      "source": [
        "top_similarity_contributions(tfidf[corpus_numeric[0]], tfidf[corpus_numeric[1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQEuR-N-1I9g"
      },
      "source": [
        "### 5.0.0 Erläutern Sie die Begriffe *Token und Tokenisierung*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf18tRw0f1MX"
      },
      "source": [
        "<bitte ausfüllen>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvjyc2tRtfWm"
      },
      "source": [
        "### 5.0.1 Erläutern Sie den Begriff *BoW* (Bag of Words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXWa_25Kf8Z-"
      },
      "source": [
        "<bitte ausfüllen>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erp77yNPtwDd"
      },
      "source": [
        "### 5.0.2 Versuchen Sie mit eigenen Worten das Prinzip des Verfahrens *TF-IDF* (Term Frequency - Inverse Document Frequency) zu erklären."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvxOBO30gCK-"
      },
      "source": [
        "<bitte ausfüllen>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4V2wRA71FWt"
      },
      "source": [
        "### 5.0.3 Artifizielle Ähnlichkeit\n",
        "\n",
        "Denken sie sich einen kurzen Text aus, der im TF-IDF-Verahren ein möglichst hohes Ähnlichkeitsmaß zum Hannover-Text aufweist, jedoch im direkten, \"manuellen\" Verleich nicht. Tragen Sie den Text in die Variable `new_text` ein."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddSifIOVXLGS"
      },
      "source": [
        "new_text = '''bitte hier Ihren Text eintragen'''\n",
        "\n",
        "tokenized_text = tokenizer(new_text)\n",
        "numeric_text = dictionary.doc2bow(tokenized_text)\n",
        "\n",
        "similarity_basic = similarity_index[numeric_text]\n",
        "similarity_tfidf = similarity_index_tfidf[numeric_text]\n",
        "\n",
        "plt.imshow(np.vstack([similarity_basic, similarity_tfidf]), cmap='hot', vmin=0)\n",
        "plt.yticks([0,1], labels=['index', 'index_tfidf'])\n",
        "plt.xticks([0,1,2,3], labels=['H', 'Bs', 'M', 'N'])\n",
        "cax = plt.colorbar()\n",
        "cax.set_label('Ähnlichkeitsmaß')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEhs0RyIgZhW"
      },
      "source": [
        "Erläutern Sie das Ergebnis.\n",
        "\n",
        "<bitte ausfüllen>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGeIfj5gRIZa"
      },
      "source": [
        "### 5.0.4 Weitere Ähnlichkeiten berechnen\n",
        "\n",
        "Definieren Sie sich drei weitere Dokumente mit jeweils mindestens 500 Wörtern. Wählen Sie dabei zwei thematisch verwandte Textquellen (z.B. zwei Ausschnitte aus dem gleichen Roman) und einen Text, der aus einer völlig anderen Quelle stammt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJz-77Szk1y6"
      },
      "source": [
        "# Dokumente definieren\n",
        "my_doc1 = ...\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsmykStn2JzU"
      },
      "source": [
        "Verfahren Sie nun mit Ihren drei Dokumenten analog zu den Schritten aus Abschnitt 5.0:\n",
        "- Numerische Repräsentation der Texte\n",
        "- Bag of words\n",
        "- Stopwords\n",
        "- TF/IDF\n",
        "\n",
        "---\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "- Der Übersichtlichkeit halber können Sie diese Aufgabe auch in einem gesonderten Notebook bearbeiten.\n",
        "- Es kann sehr hilfreich sein, jeden Schritt bzw. kleine Gruppen von Anweisungen zunächst in einem Kommentar zu beschreiben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wg4BMhLX2Qre"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXPJPt4l2WzN"
      },
      "source": [
        "![insitubytes](https://drive.google.com/uc?id=1EAJK7AI9tcZRo3VvYq7vEKGxk7vmK2Ff)"
      ]
    }
  ]
}