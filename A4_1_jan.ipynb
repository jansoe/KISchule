{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4_1_jan.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jansoe/KISchule/blob/main/A4_1_jan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX02S6Aq8OxF"
      },
      "source": [
        "# 4. Generative Netzwerke\n",
        "\n",
        "#### Teil 2: [*Generative Adversarial Networks*](https://de.wikipedia.org/wiki/Generative_Adversarial_Networks) (GANs), bei denen ein *generatives* Netz versucht, ein *diskriminatives* Netz auszutricksen.\n",
        "\n",
        "Das Verfahren ist wie gehabt:\n",
        "- Erstellen Sie eine Kopie dieses Notebooks in ihrem Google Drive (vorgeschlagene Umbenennung: \"A4 - Vorname, Nachname\")\n",
        "- Editieren Sie die Text- und Codezellen.\n",
        "- Schicken Sie einen Link zum Kommentieren Ihres Notebook an\n",
        "\n",
        "---\n",
        "\n",
        "**Anmerkungen**\n",
        "\n",
        "- Analog zum ersten Teil verwenden wir tensorflow in der aktuellen Version 2. Solange diese Version nicht standardmäßig in Colab importiert wird, müssen wir dies explizit angeben:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABcCVouWB9fx"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKgoIQgiDQ0G"
      },
      "source": [
        "import tensorflow as tf\n",
        "# Es lohnt sich zu überprüfen, ob man auch wirklich eine GPU bekommen hat.\n",
        "print(\"Anzahl der zugewiesenen GPUs:\", len(tf.config.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6sSUrDuDXkX"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j_Db4FUKZ10"
      },
      "source": [
        "## 4.1 Generative Adverserial Networks (GANs)\n",
        "\n",
        "Mit den Autoencodern aus Abschnitt 4.0 haben Sie Ihr bereits eine Art von generativem Netz erstellt. Eine weitere Klasse von generativen Algorithmen, die in letzter Zeit für Aufsehen gesorgt haben, sind die sogenannten *Generative Adversarial Networks*. Mit solchen Netzwerken wurde z.B. auch das Portrait von [Edmond de Belamy](https://de.wikipedia.org/wiki/Portrait_of_Edmond_de_Belamy) erzeugt.\n",
        "\n",
        "Bei dieser Technik handelt es sich um zwei Netzwerke, die gegeneinander ausgespielt werden. Ein *generatives* Netzwerk erstellt künstliche Bilder, während ein *diskriminatives* Netzwerk zu klassifizieren versucht, ob ein Bild aus einem Trainingsset von *echten* Bildern stammt oder vom generativen Netzwerk erzeugt wurde - also so etwas wie ein Turing-Test für Automaten. Die beiden Netzwerke werden dabei gleichzeitig trainiert und schaukeln sich gegenseitig hoch.\n",
        "\n",
        "Ein weiteres fastzinierendes Beispiel für den Einsatz dieser Technik sind [künstlich generierte, fotorealistische Bilder von Menschen, die es nicht gibt](https://thispersondoesnotexist.com/).\n",
        "\n",
        "### 4.1.0 Datensatz für ein Anwendungsbeispiel\n",
        "\n",
        "Leider ist das Trainieren von GANs auf echten Bilddaten sehr rechenintensiv. Daher müssen wir für unser Beispiel wieder mit dem MNIST-Datensatz Vorlieb nehmen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuSzCk3RaDM4"
      },
      "source": [
        "# Herunterladen des Datensatzes\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "# Hinzufügen einer weiteren Dimension für Convolutional Networks\n",
        "train_images = train_images[:,:,:,np.newaxis].astype('float32')\n",
        "\n",
        "# Normalize the images to [-1, 1]\n",
        "train_images_gan = (train_images - 127.5) / 127.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoDc3cGrDm-V"
      },
      "source": [
        "# Hier erstellen wir ein Objekt, das die Portionierung der Daten während des Trainings handhabt.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images_gan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeGa3cnHdQkf"
      },
      "source": [
        "### 4.1.1 Das Generator Modell\n",
        "\n",
        "Analog zum Generator im Autoencoder erstellen wir hier auch wieder ein Modell, das aus einem kompakten Eingabecode Ausgabebilder erzeugt. Die Anzahl der Dimensionen der zugehörigen latenten Variable setzen wir diesmal jedoch auf 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ6CMvSdG-hL"
      },
      "source": [
        "noise_dim = 8\n",
        "\n",
        "generator = tf.keras.Sequential()\n",
        "\n",
        "generator.add(layers.Dense(7*7*256, use_bias=False, input_shape=(noise_dim,)))\n",
        "generator.add(layers.BatchNormalization())\n",
        "generator.add(layers.LeakyReLU())\n",
        "\n",
        "generator.add(layers.Reshape((7, 7, 256)))\n",
        "\n",
        "generator.add(layers.Conv2DTranspose(128, kernel_size=(5, 5), strides=1, padding='same', use_bias=False))\n",
        "generator.add(layers.BatchNormalization())\n",
        "generator.add(layers.LeakyReLU())\n",
        "\n",
        "generator.add(layers.Conv2DTranspose(64, kernel_size=(5, 5), strides=2, padding='same', use_bias=False))\n",
        "generator.add(layers.BatchNormalization())\n",
        "generator.add(layers.LeakyReLU())\n",
        "\n",
        "generator.add(layers.Conv2DTranspose(1, kernel_size=(5, 5), strides=2, padding='same', use_bias=False, activation='tanh'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wv-EvxjQLAbT"
      },
      "source": [
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVu9tnUK4i5s"
      },
      "source": [
        "Wie zuvor können wir den Generator mit Rauschen füttern. Da er noch nicht trainiert ist, bekommen wir auch nur Rauschen zurück:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU_PnXljH1zk"
      },
      "source": [
        "np.random.seed(42)\n",
        "noise = tf.random.normal([1, noise_dim])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wM6-J0_dyEk"
      },
      "source": [
        "### 4.1.2 Der Diskriminator\n",
        "\n",
        "Der Diskriminator ist ein einfaches Klassifikationsnetz, wie wir es bereits aus vorangegangenen Aufgaben kennen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlUgRR5cH5oN"
      },
      "source": [
        "discriminator = tf.keras.Sequential()\n",
        "discriminator.add(layers.Conv2D(64, (5, 5), strides=2, padding='same', input_shape=[28, 28, 1]))\n",
        "discriminator.add(layers.LeakyReLU())\n",
        "discriminator.add(layers.Dropout(0.3))\n",
        "\n",
        "discriminator.add(layers.Conv2D(128, (5, 5), strides=2, padding='same'))\n",
        "discriminator.add(layers.LeakyReLU())\n",
        "discriminator.add(layers.Dropout(0.3))\n",
        "\n",
        "discriminator.add(layers.Flatten())\n",
        "discriminator.add(layers.Dense(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fVx0v7AQWbq"
      },
      "source": [
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewj7yrbA48sy"
      },
      "source": [
        "Um zu sehen, ob wir die Dimensionen richtig hinbekommen haben, generieren wir eine Vorhersage für das eben generierte Bild."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4ryYFfmINtH"
      },
      "source": [
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlTZtV148sYr"
      },
      "source": [
        "Zur Kontrolle füttern wir dem Modell nun noch eines der echten Bilder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znTwQ4S08q42"
      },
      "source": [
        "decision = discriminator(train_images[:1])\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8zOW3sY5QCA"
      },
      "source": [
        "Der Diskriminator funktioniert, ist sich aber aufgrund seiner Unerfahrenheit noch unsicher, was wir von ihm wollen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B27XtJHMeRJw"
      },
      "source": [
        "### 4.1.3 Fehlerdefinition\n",
        "\n",
        "Bisher haben wir zwei Netzwerke erstellt, zwischen denen noch kein Zusammenhang besteht. Dieser wird durch die Berechnung der Fehler hergestellt. Hierfür ist es hilfreich, zunächst den grundsätzlichen Ablauf des Trainings kennenzulernen.\n",
        "\n",
        "In jedem Trainingsschritt wird für ein Batch (also eine kleine Portion) von \"echten\" Bildern (Bilder des Trainingsdatensatzes) zunächt ein gleichgroßer Stapel \"falscher\" Bilder vom Generator erzeugt (`g_out`). Für beide Arten von Bildern - das \"reale\" Bild des Trainingsdatensatzes und das generierte Bild - werden Antworten des Diskriminator-Netzwerks errechnet (`d_out_real`, `d_out_gen`). Aus dieser Vorhersage wird dann für beide Netze ein Fehler und damit eine Gewichtsveränderung zur Minimierung des jeweiligen Fehlers berechnet:\n",
        "\n",
        "**Generator**:\n",
        "- `1 - d_out_gen`: Je niedriger der Diskriminator auf das generierte Bild reagiert, desto größer der Fehler des Generators.\n",
        "\n",
        "**Diskriminator**: (Der Fehler des Diskriminators ergibt sich aus zwei Werten)\n",
        "- `1 - d_out_train`: Je niedriger der Output für die Bilder des Trainingsdatensatzes, desto höher ist der Fehler.\n",
        "- `d_out_gen`: Je höher der Output für die Bilder des Generators, desto höher ist der Fehler.\n",
        "\n",
        "Da die Fehlerart von Generator und Diskriminator die gleiche wie bei einem gewöhnlichen binären Klassifikationsproblem ist, können wir diese für beide gemeinsam definieren:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0glw4tvIgYI"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMEvaRUg6XH9"
      },
      "source": [
        "Die Fehlerfunktion des Generators wird wie beschreiben auf dem Output des Diskriminators berechnet. Ziel des Generators ist es, den Diskriminator davon zu überzeugen, dass alle seine Bilder *einsen* sind:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LheY9YfdJSNL"
      },
      "source": [
        "# Der Generator soll künstliche Bilder produzieren, so dass der Diskriminator den Generator-Output mit möglichst hoher Wahrscheinlichkeit als echt klassifiziert \n",
        "def generator_loss(d_out_gen):\n",
        "    generator_loss = cross_entropy(tf.ones_like(d_out_gen), d_out_gen)\n",
        "    return generator_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33vW-3G_avjI"
      },
      "source": [
        "Der Diskriminator soll lernen, die Wahrscheinlichkeit dafür auszugeben, dass ein Bild \"echt\", d.h. nicht generiert ist. Er reagiert also idealerweise mit einer 1 auf ein Bild des Trainingsdatensatzen und mit einer 0 auf ein vom Generator-Netzwerk generiertes Bild."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGo-Y94oJIIl"
      },
      "source": [
        "def discriminator_loss(d_out_real, d_out_gen):\n",
        "    real_loss = cross_entropy(tf.ones_like(d_out_real), d_out_real)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(d_out_gen), d_out_gen)\n",
        "    discriminator_loss = real_loss + fake_loss\n",
        "    return discriminator_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ose4Ckb6p9e"
      },
      "source": [
        "Ähnlich wie beim *image style transfer* aus A3 definieren wir uns hier wieder einen eigenen Trainingsschritt, der den oben beschriebenen Prozess des Trainings umsetzt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfOUJ0hoKOkS"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "@tf.function # mit diesem Zusatz können wir das Training etwas beschleunigen\n",
        "def train_step(images):\n",
        "    \n",
        "    noise = tf.random.normal([images.shape[0], noise_dim])\n",
        "    # Trainings des Diskriminators\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "        generator_out = generator(noise, training=False)\n",
        "        discriminator_out_real = discriminator(images, training=True)\n",
        "        discriminator_out_generated = discriminator(generator_out, training=True)\n",
        "\n",
        "        disc_loss = discriminator_loss(discriminator_out_real, discriminator_out_generated)\n",
        "\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    \n",
        "    # Training des Generators\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        generator_out = generator(noise, training=True)\n",
        "        discriminator_out_generated = discriminator(generator_out, training=False)\n",
        "\n",
        "        gen_loss = generator_loss(discriminator_out_generated)\n",
        "        \n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    \n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItpzgYOJ7e7F"
      },
      "source": [
        "Wir generieren uns einige Werte für die latente Variable, um den Trainingsfortschritt zu visualisieren: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNH7XwA2aOCt"
      },
      "source": [
        "# We will reuse these random codes overtime to visualize progress during training\n",
        "test_input = tf.random.normal([16, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PvkzFJZK1R6",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown Bitte ausführen: Code zum Visualisieren des Trainings\n",
        "\n",
        "def generate_and_show_images(model, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode\n",
        "  # dropouts: rescaled and no dropout\n",
        "  # batchnorm: frozen\n",
        "  # other layers: weights frozen\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8hwd1Qw72m3"
      },
      "source": [
        "Dann beginnt die eigentliche Trainingsschleife:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BBO5XSCKj5R"
      },
      "source": [
        "train_dataset = train_dataset.shuffle(60000)\n",
        "train_dataset = train_dataset.batch(256)\n",
        "\n",
        "for epoch in range(50):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in train_dataset:\n",
        "        train_step(image_batch)\n",
        "\n",
        "    # Produce images as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_show_images(generator, test_input)\n",
        "\n",
        "    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIlTJsl4fd8g"
      },
      "source": [
        "### 4.1.4 Ergebnis\n",
        "\n",
        "Nach einigen Iterationen erzeugt der Generator Bilder, die wie handgeschriebene Zahlen oder zumindest Symbole aussehen. Beim Autoencoder bildeten sich bestimmte Bereiche im latenten Raum heraus, an denen einzelne Ziffern repräsentiert werden. Diese Bereiche konnten wir finden, indem wir sie mit dem Encoder *aktivieren*. Beim Generator der GANs ist das nicht mehr möglich - er generiert zufällige Bilder, ohne dass wir direkten Einfluss nehmen können. \n",
        "\n",
        "Generieren Wir zum Spaß also noch einige Zufallsbilder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYq2A1W6JFkd"
      },
      "source": [
        "for i in range(5):\n",
        "    plt.subplot(1,5,i+1)\n",
        "    random_code = np.random.randn(1, noise_dim) # zufälliger Code!\n",
        "    image = generator.predict(random_code)\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdCDjvo2-IV0"
      },
      "source": [
        "### 4.1.5 Aufgabe: Größe des latenten Raumes\n",
        "\n",
        "Im vorangegangenen Beispiel hatte unser latenter Raum eine Dimensionalität von 8 (Variable `latent_dim`, definiert in Abschnitt 4.1.0), analog zum Autoencoder aus Abschnitt 4.0, der auch mit einer Code-Länge von acht Zahlen zurecht kam. Da es bei GANs nicht auf Kompression ankommt, wird hier oft ein vergleichsweise großer Raum gewählt, mit der Hoffnung, reichhaltigere Ergebnisse zu erziehlen. Verändern Sie die Variable `latent_dim` und führen Sie den Code ab dieser Zelle erneut aus und untersuchen Sie folgende Fragestellungen:\n",
        "\n",
        "- Generiert das Modell für `latent_dim=100` wesentlich bessere Ergebnisse? \n",
        "- Generiert das Modell auch noch für `latent_dim=2` vernünftige Ergebnisse?\n",
        "- Ab welcher Größe funktioniert es kaum noch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY7MraDt-3ht"
      },
      "source": [
        "### 4.1.6 OPTIONAL: Morphing\n",
        "\n",
        "In Abschnitt 4.1.4 haben Sie gesehen, wie wir aus zufälligen Codes Bilder generieren können. Kopieren Sie den Code zum Morphen zwischen Bildern des Autoencoders aus Abschnitt 4.0.3 und modifizieren Sie ihn so, dass zwischen zwei zufällig generierten Codes des GAN-Generators gemorpht wird."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDCklf_1ldWK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZECoQiR6_hKr"
      },
      "source": [
        "# Lösung:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqsg7-h3lcvC"
      },
      "source": [
        "![insitubytes](https://drive.google.com/uc?id=1EAJK7AI9tcZRo3VvYq7vEKGxk7vmK2Ff)"
      ]
    }
  ]
}